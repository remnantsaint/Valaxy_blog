---
categories: 
  - 人工智能
  - 机器学习
cover: 
date: 2024-09-06 10:52:42
image: 
layout: post
tags: 机器学习
time_warning: true
title: Logistic回归 
top: 
---

## Logistic回归概念
### 概述
&emsp; Logistic回归又叫逻辑回归，实际上是用来做分类，主要思想是根据现有数据对分类边界线建立回归公式，以此进行分类

### Sigmoid函数
#### 回归概念
&emsp; 假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合过程就叫做回归。进而可以得到对这些点的拟合直线方程，以下是根据这个回归方程如何进行分类。

#### 二值型输出分类函数
&emsp; 我们需要一个能接受所有输入，然后预测出类别的函数，例如在两个类的情况下，函数只输出0 or 1，这种函数称为`海维赛德阶跃函数`，或者直接称为`单位阶跃函数`。  
&emsp; 然而这个函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。而`Sigmoid函数`在数学上更容易处理，具体公式如下：
$\sigma (z)=\frac{1}{1+e^{-z} }$   
&emsp; 下图给出了Sigmoid函数在不同坐标尺度下的两条曲线图。当x为0时，Sigmoid函数值为 0.5，随着 x 的增大，对应的 Sigmoid 值将逼近于 1；而随着 x 的减小，Sigmoid 的值将逼近于0，如果横坐标刻度足够大，Sigmoid 函数看起来很像一个阶跃函数。
![](https://cdn.jsdelivr.net/gh/remnantsaint/hexoImage@main/%E9%98%B6%E8%B7%83.png)  
&emsp; 为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有结果的值相加，将这个总和带入 Sigmoid 函数中，得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 被分入 0 类，因此 Logistic 回归也是一种概率估计。

### 基于最优化方法的回归系数确定
&emsp; 将 Sigmoid 函数的输入记为 z ，由下面的公式得到：  
$$z = w_{0}x_{0} + w_{1}x_{1} + ......+w_{n}x_n$$  
&emsp; 如果采用向量的写法，上述公式可以写成$z=w^{T}x$，它表示将这两个数值向量对应的元素相乘然后全部加起来得到 z 值。其中向量 x 是分类器的输入数据，向量 w 是我们要找的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识，这里我们使用——梯度上升法（Gradient Ascent）。

### 梯度上升法
#### 梯度的介绍
- 向量 = 值 + 方向
- 梯度 = 向量
- 梯度 = 梯度值 + 梯度方向























## Reference
<https://github.com/remnantsaint/ailearning/blob/master/docs/ml/5.md>
