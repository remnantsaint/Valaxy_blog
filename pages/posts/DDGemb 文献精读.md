---
layout: post
title: DDGemb 文献精读
date: 2025-09-22 11:48:57
updated: 2025-09-22
time_warning: true 
cover: 
top: 
tags: 
 - ddg
categories: 
 - 深度学习
 - 文献精读
# author: @Remsait
---
# DDGemb：利用嵌入和深度学习预测单点和多点变异对蛋白质稳定性的影响
  [DDGemb: predicting protein stability change upon singleand multi-point variations with embeddings and deep learning](https://academic.oup.com/bioinformatics/article/41/1/btaf019/7952013)
## 摘要
  动机：了解氨基酸残基变异对蛋白质稳定性的影响，是功能性蛋白质设计以及理解蛋白质变异如何导致疾病发生的重要步骤。计算方法对于补充实验手段、实现对大量变异数据集的快速筛选至关重要。

  结果：在本研究中，我们提出了DDGemb，这是一种新颖的方法，结合了蛋白质语言模型嵌入和Transformer架构，用于预测单点和多点变异引起的蛋白质ΔΔG变化。DDGemb在一个来源于文献的高质量数据集上进行了训练，并在现有的单点和多点变异基准数据集上进行了测试。DDGemb在单点和多点变异预测方面均达到了最先进的水平。
## 引言
  预测编译对蛋白质热力学稳定性影响的计算方法，在计算蛋白质设计、蛋白质变体的功能表征及其与疾病发生的关系中发挥着根本性的作用。近年来，已提出多种用于预测突变引起的蛋白质稳定性变化的方法（(ΔΔG)。

  现有的工具大致可根据其依赖的信息类型（蛋白质结构和/或序列）以及进行预测的方法类型进行分类。基于结构的方法需要以蛋白质结构作为输入。已提出了不同的基于结构的预测方法，包括基于力场和能量函数的方法、传统机器学习方法、深度学习方法

  基于序列的方法仅使用可从蛋白质序列中提取的特征。迄今为止，绝大多数可用的方法都基于传统的特征，例如进化信息和理化性质，并通过传统的机器学习方法进行处理。ACDC-NN-Seq 引入了深度学习方法（卷积网络）来处理从多序列比对中提取的序列谱。最近，PROSTATA 采用蛋白质语言模型对蛋白质野生型和突变序列进行编码，然后在 PROSTATA 中使用一个仅含单个隐藏层的简单神经网络处理该蛋白质语言模型的输入。基于序列的 THPLM 采用了预训练的蛋白质语言模型和一个简单的卷积神经网络。最后，ThermoMPNN 也采用了一种名为 ProteinMPNN 的预训练蛋白质语言模型，并结合深度网络来预测单点变异引起的 ΔΔG。

  蛋白质稳定性预测领域的一个主要挑战是能够预测多点变异下的ΔΔG，即当多个残基位置同时发生变异时，对蛋白质稳定性的影响。迄今为止，仅有少数方法支持将多点变异作为输入：四种基于结构的方法 FoldX、MAESTRO、DDGun3D、Dynamut2 以及一种基于序列的方法 DDGunSeq。总体而言，现有方法在预测多点变异 ΔΔG 方面的性能通常低于其在单点变异上的表现

  在本研究中，我们提出了一种名为 DDGemb 的新方法，用于预测蛋白质在单点和多点变异下的 ΔΔG。DDGemb 利用 ESM2 蛋白质语言模型来表示蛋白质及其变异，并结合基于 Transformer 编码器的深度学习架构来预测 ΔΔG。

  我们使用 S2648 数据集中的全长蛋白质序列和单点变异来训练 DDGemb，该数据集此前已被用于训练多种先进的方法。我们在单点和多点变异的 ΔΔG 预测任务上评估了 DDGemb 的性能。对于单点变异，我们采用了近期文献中提出并已广泛用于多种工具性能评估的 S669 数据集。对于多点变异，我们采用了一个源自 PTmul 数据集的数据集。在这两个基准测试中，DDGemb 均表现出最先进的性能，超越了现有的基于序列和基于结构的方法。
## 材料和方法
### 数据集
#### S669 盲测数据集
  为了对 DDGemb 的性能进行公平且全面的评估，并与其他最先进的方法进行比较，我们使用了一个在文献中已被用于评估大量现有蛋白质稳定性变化预测工具的独立数据集

  该数据集名为 S669，包含发生在 95 条蛋白质链上的 1338 个正向和反向单点突变。ΔΔG 值来自 ThermoMutDB，并由作者手动核对。本文采用负值 ΔΔG 表示导致蛋白质结构不稳定的变异这一惯例。值得注意的是，该数据集在序列一致性低于 25% 的条件下，与文献中常用的训练数据集（包括 S2648 和 VariBench 数据集无冗余。这使得与大多数先进工具的比较更加公平。S669 中的变异是基于 PDB 链提供的。在本研究中，由于 DDGemb 使用蛋白质语言模型进行输入编码，我们利用 SIFTS 将所有变异映射到全长 UniProt 序列上。
#### S2450 训练集
  为了构建我们的训练集，我们从著名的、被广泛使用的 S2648 数据集出发。该数据集包含在 131 种不同蛋白质上的 2648 个单点突变。相关的实验 ΔΔG 值来自 ProTherm 数据库，并经过人工检查和修正，以避免不一致的情况。

  与之前使用同一数据集的研究不同，那些研究是将突变直接映射到 PDB 链的序列上，而本研究采用的是来自 UniProt 的全长蛋白质序列。为此，我们使用了 SIFTS 工具，将 PDB 链及其突变位置映射到对应的 UniProt 全长序列上。（作者用的是训练好的 ESM  ，  输入 UniProt 的全长蛋白质序列

  S669 数据集相对于 S2648 的同源性降低最初在 (Pancotti C, Benevenuta S, Birolo G et al. Predicting protein stability changes upon single-point mutation: a thorough comparison of the available tools on a new dataset. Brief Bioinform 2022;23:bbab555.) 中进行，但当时仅考虑了序列中被 PDB 覆盖的部分。这种方法无法保证检测到全长序列上的所有序列相似性。因此，在本研究中，我们直接比较了 S669 和 S2648 中的 UniProt 全长序列，并从训练集中移除了那些与测试集（S669）中任意序列的序列同一性超过 25% 的序列。总共从 S2648 中移除了 18 条序列，共计 198 个单点突变。这个经过缩减的数据集在本文中统称为 S2450。（作者的意思是之前的研究只考虑了 PDB 覆盖的序列部分的同源降低，而 PDB 覆盖的部分只是被实验测定出三维结构的一部分，作者用全长序列来比较

  本研究采用 S2450 数据集进行 5 折交叉验证。为此，我们实施了文献 (Fariselli P, Martelli PL, Savojardo C et al. INPS: predicting the impact of non-synonymous variations on protein stability from sequence. Bioinformatics 2015;31:2816–21.) 中描述的严格数据划分方法：将同一蛋白质上的所有突变归入同一个交叉验证子集，并在子集之间分配蛋白质时，考虑蛋白质序列间的两两相似性（设定 25% 为序列同一性阈值）。通过这种方式，在交叉验证过程中，训练集和验证集所包含的蛋白质序列之间不存在任何冗余。（作者用蛋白质来划分折，一个蛋白质的所有突变在同一折里，保证任意两个序列相似度超过 25% 的蛋白质，不会被分到不同的折中

  S2450 数据集在构建时，倾向于包含更多导致蛋白质不稳定的变异（即 ΔΔG 值为负的变异），因此数据分布是不平衡的。为了平衡数据集、减少对不稳定 ΔΔG 值的预测偏差，并提高模型对稳定化变异的预测能力，我们利用了变异的热力学可逆性原理，即 ΔΔG(A → B) = −ΔΔG(B → A) (Capriotti 等, 2008)。利用这一可逆性特性，可以人为地将变异集合加倍，加入其反向变异，并将实验测得的 ΔΔG 值符号取反。
#### 多点变异：缩减后的 PTmul 数据集
  我们还采用了一个数据集，用于测试 DDGemb 在多点变异情况下预测 ΔΔG 的性能。该数据集称为 PTmul，由 Montanucci 等人（2019）首次提出：它包含了在 91 种蛋白质上的 914 个多点变异。

  然而，原始的 PTmul 数据集与我们的 S2450 训练数据集相比，存在较高的序列相似性。为了对模型性能进行公平评估，我们从 PTmul 中排除了所有与 S2450 训练集中任何蛋白质序列相似的蛋白质。

  经过这一缩减步骤后，我们保留了在 14 种蛋白质上发生的 82 个多点变异。尽管与原始数据集相比，变异数量显著减少，但通过这种同源性降低（homology reduction）处理，确保了对不同方法的评估是公平的。这个经过缩减的数据集被称为 PTmul-NR（NR 可能代表 "Non-Redundant"，即“非冗余”）。
## DDGemb 方法
  下图展示了DDGemb深度学习模型的概览。该架构包含两个组成部分：(i) 输入编码和 (ii) ΔΔG 预测模型。
<img src="https://cloudflare.remsait.com/img/DDGEmb202509221445950.png"  alt="404" title=""  />
<center><div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      模型架构
  	</div>
</center>
### Input encoding
  为了编码单个氨基酸残基的变异，我们从野生型和变异型蛋白质序列开始。第一步，将这两个长度均为 L 的序列，使用 ESM2 蛋白质语言模型（pLM）进行编码。在评估了多种可用模型并优化了输入编码（见第3节）后，本文采用了在 UniRef50 数据库上训练的、具有 33 层的中等规模模型，该模型包含 6.5 亿（650M）个参数。该模型能生成维度为 1280 的残基级别嵌入向量，在表征能力与计算需求之间达到了良好的平衡。生成嵌入向量所使用的 ESM2 工具包来自 [https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm) 。

  应用 ESM2 蛋白质语言模型（pLM）后，会得到两个大小为 L × d 的矩阵，分别记为 Ewt 和 Evt，它们代表了野生型序列和变异型序列（由单点或多点变异产生）的残基级别嵌入向量。随后，通过计算 Ewt 和 Evt 的逐元素（element-wise）差值，生成一个单一的 L × d 矩阵 D 来编码该变异：
  $$D = E_{wt} - E_{vt}$$
  该矩阵 D 将作为下游 ΔΔG 预测架构的输入。
### 基于 Transformer 的 ΔΔG 预测网络
  DDGemb架构的其余部分旨在从编码了蛋白质突变的输入矩阵D出发，预测出一个ΔΔG值。最终模型的超参数是根据表1中报告的不同配置，通过交叉验证进行优化确定的。在完成优化后，最终选定的模型是 model4 ，其结构如下所述。表1 如下：  
  
| Model  | Model configuration                  | PCC         | RMSE        | MAE         |
|--------|---------------------------------------|-------------|-------------|-------------|
| Model0 | m = 32,  h = 2,  s = 128              | 0.68 ± 0.01 | 1.27 ± 0.10 | 0.97 ± 0.09 |
| Model1 | m = 64,  h = 2,  s = 256              | 0.70 ± 0.01 | 1.23 ± 0.11 | 0.94 ± 0.09 |
| Model2 | m = 64,  h = 4,  s = 256              | 0.70 ± 0.01 | 1.24 ± 0.11 | 0.96 ± 0.09 |
| Model3 | m = 128, h = 4,  s = 512              | 0.70 ± 0.01 | 1.24 ± 0.11 | 0.96 ± 0.09 |
| Model4 | m = 128, h = 8,  s = 512              | 0.71 ± 0.01 | 1.23 ± 0.10 | 0.94 ± 0.08 |
| Model5 | m = 256, h = 8,  s = 1024             | 0.71 ± 0.02 | 1.23 ± 0.12 | 0.95 ± 0.10 |

  输入矩阵 D 首先经过一个一维卷积层（1D convolution layer）进行处理，该卷积层包含 m = 128 个大小为 w = 15 的滤波器（filters），并使用 ReLU 激活函数。该一维卷积层通过一系列宽度为 w 的滑动滤波器，将高维度的输入数据投影到大小为 m 的低维空间，从而提取局部上下文信息。一维卷积层的输出是一个维度为 L × m 的矩阵 C。 （每个卷积核权重不同，128个一维卷积核
  
  矩阵 C 随后被输入到一个 Transformer 编码器层（Vaswani 等，2017）中。该编码器层采用级联结构，包含一个具有八个注意力头（h）的多头注意力层、残差连接以及一个逐位置的前馈神经网络（FFN）。Transformer 编码器负责计算输入序列上的自注意力，生成能够综合考虑输入序列中不同位置之间相互关系的输出表示。
  
  此处采用的架构直接源自原始的 Transformer 定义。形式上，给定维度为 L × m 的输入序列 C，多头注意力层的每个头 i 会使用三个可学习的权重矩阵，分别称为 $A_Q^i$（查询矩阵）、 $A_K^i$（键矩阵）和 $A_V^i$（值矩阵），每个矩阵的维度均为 m × r；其中 r = m/h（在本例中 r = 16），h 为注意力头的数量（此处设为 8）。输入矩阵 C 首先通过 $A_Q^i$、$A_K^i$ 和 $A_V^i$ 进行如下投影：（矩阵QKV 128 × 16 ，C是 L × 128   
  $$Q^i = C \cdot A_Q^i$$
  $$K^i = C \cdot A_K^i$$
  $$V^i = C \cdot A_V^i$$
  （输出的矩阵QKV^i L × 16，L是蛋白质序列长度）其中，$\cdot$ 表示矩阵乘法运算符。然后，对每个注意力头 i 计算一个维度为 L × r 的注意力输出 $Z^i$ ，计算方式如下：（结果是 L×L 点积 L×r，因此每个注意力头 i 的输出$Z^i$的维度是 L × r  
  $$Z^i = softmax(\frac{Q^i \cdot (K^i)^T}{\sqrt{r}}) \cdot V^i$$
  来自不同注意力头的各个 Zi 随后被拼接起来，与一个维度为 m × m 的输出权重矩阵 $A_O$ 相乘，然后将结果通过残差连接加到输入矩阵 C 上。（拼接后是 L × 128，即 L × m  
  $$Z = [Z^1, Z^2,....., Z^h] \cdot A_O + C$$
  其中 [] 表示按特征维度拼接操作，得到的输出矩阵 Z 的维度为 L × m（与输入矩阵 C 的维度相同
  
  最终的 Transformer 编码器输出 F 是一个维度为 L × m 的矩阵，通过将一个逐位置的前馈神经网络（FNN）独立地应用于矩阵 Z 的每个位置（1 <= j <= L ），并添加残差连接得到。换句话说，F 的每一行 $f_j$ 按如下方式计算：
  $$f_j = FFN(z_j) + z_j = ReLU(z_j \cdot W_1 + b_1) \cdot W_2 + b_2 + z_j$$
  其中，ReLU 是激活函数，定义为 g(x) = max(0, x) ，$W_1, b_1, W_2, b_2$ 分别是前馈神经网络中与位置无关的权重参数和偏置项，其维度分别为 m×s, 1×s, s×m, 1×m，本文中，我们将逐位置前馈网络隐藏层的维度 s 设为 512
  
  Transformer 编码器的输出矩阵 F 随后通过全局平均池化和全局最大池化层被压缩为两个一维向量，分别记为 pave 和 pmax ，这两个池化操作作用于矩阵 F 的第一个维度 L （此时 F 表示序列中每个氨基酸位置的高级特征，但是要预测的是一个值，所以用池化把 L 行压缩成 1 行  
  $$p_{ave}(F) = (1/L\sum_{j = 1}^{L} f_{j1},....,1/L\sum_{j = 1}^{L} f_{jm})$$
  $$p_{max}(F) = (max_jf_{j1},...., max_jf_{jm})$$
  然后将这两个池化后的向量拼接起来，形成一个大小为 2m 的单一向量 P ：  
  $$P = [p_{ave}(F), p_{max}(F)]$$
  其中 [] 表示拼接操作。最终，向量 P 经过一个由权重向量 $W^O$ 和偏置项 $B^O$ 参数化的线性前馈网络（FFN）处理，输出预测的 ΔΔG 值 $\hat{y}$：
  $$\hat{y} = P \cdot w^O + b^O$$
### 模型训练与实现
  给定一个包含 N 个蛋白质单残基突变的数据集 $D = \{D^1, D^2, ...., D^N\}$，其对应的目标 ΔΔG 值为 $y = \{y^1, y^2, ...., y^N\}$，以及模型的预测值$\hat{y} = \{\hat{y^1}, \hat{y^2},...., \hat{y^N}\}$，模型的训练通过在训练数据上最小化均方误差（MSE）损失函数来进行：
  $$MSE(Y,\hat{Y})=\frac{1}{N}\sum_{j=1}^{N}(y^i-\hat{y}^i)^2$$
  优化过程采用梯度下降法，并用 Adam 优化器。训练数据被划分为大小为 128 的小批量（mini-batches）。模型训练共进行 500 个 epoch，并在验证集（从训练集中划分出的子集）上的误差开始上升时提前停止训练（early stopping）。
## 评估指标
  为了评估不同方法的性能，我们采用以下几种广泛使用的评估指标。下文中，e 和 p 分别表示实验测得和模型预测的 ΔΔG 值，而 pdir 和 pinv 分别表示正向编译和相应反向变异的预测 ΔΔG 值。 e 与 p 之间的皮尔逊相关系数（PCC）定义为：
  $$\mathrm{PCC}(e, p) = 
\frac{\sum_{i=1}^{N} (e_i - \bar{e})(p_i - \bar{p})}
{\sqrt{\sum_{i=1}^{N} (e_i - \bar{e})^2} \, 
 \sqrt{\sum_{i=1}^{N} (p_i - \bar{p})^2}}$$
  其中 e 和 p 分别为实验测得的和模型预测的 ΔΔG 值的平均值。e 与 p 之间的均方根误差（RMSE）定义如下：  
  $$\mathrm{RMSE} = 
\sqrt{\frac{1}{N} \sum_{i=1}^{N} (e_i - p_i)^2}$$
  e 和 p 之间的平均绝对误差（MAE）定义为：
  $$\mathrm{MAE} = 
\frac{1}{N} \sum_{i=1}^{N} \lvert e_i - p_i \rvert$$
  为了评估不同工具的反对称性特性，我们采用了文献中定义的两个额外指标。$p^{dir}$ 和 $p^{inv}，称为 $r_{d-r}$：
  $$r_{d-r} = r(p^{dir}, p^{inv})$$  
  反对称偏差 $<\delta>$ 定义如下：
  $$\langle \delta \rangle = \frac{ {\textstyle \sum_{i=1}^{N}}(p_i^{dir} - p_i^{inv}) }{2N}$$
## 结果
### 在 S2450 数据集上的交叉验证结果
  在首次实验中，我们在 S2450 数据集上进行了 5 折交叉验证。为此，我们采用 Fariselli 等人（2015）提出的最严格的数据划分方法，即将同一蛋白质上的所有突变保留在同一个交叉验证子集中，并将序列相似度超过 25% 的蛋白质也划分到同一子集中。序列比对使用的是全长 UniProt 序列。
  
  综合考虑平均皮尔逊相关系数（PCC）、均方根误差（RMSE）和平均绝对误差（MAE）及其对应的标准差，性能最优的模型是 Model4 架构。该架构包含 128 个一维卷积滤波器、8 个 Transformer 编码器注意力头，以及 Transformer 编码器前馈网络（FFN）中 512 个隐藏单元（见表 1）。因此，我们选择该配置作为最终模型。
### 在 S669 数据集上对单点突变的预测
  我们使用通用基准数据集 S669，将 DDGemb 与近年来提出的多种先进方法进行了比较。21 种不同方法的结果来自 (Pancotti et al. 2022)，除了本文提出的 DDGemb、PROSTATA (Umerenkov et al. 2023)、THPLM (Gong et al. 2023) 和 ThermoMPNN (Dieckhaus et al. 2024)，这些方法的结果取自其各自的论文。
  
  参与比较的方法包括九种基于序列的预测工具，即 INPS (Fariselli et al. 2015)、ACDC-NN-Seq (Pancotti et al. 2022)、DDGun (Montanucci et al. 2019)、I-Mutant3-Seq (Capriotti et al. 2005)、SAAFEC-SEQ (Li et al. 2021)、MUPro (Cheng et al. 2006)、PROSTATA (Umerenkov et al. 2023)、THPLM (Gong et al. 2023) 和 ThermoMPNN (Dieckhaus et al. 2024)
  
  以及十五种基于结构的方法，即 ACDC-NN (Benevenuta et al. 2021)、PremPS (Chen et al. 2020)、DDGun3D (Montanucci et al. 2019)、INPS-3D (Savojardo et al. 2016)、ThermoNet (Li et al. 2020)、MAESTRO (Laimer et al. 2015, 2016)、Dynamut (Rodrigues et al. 2018)、PoPMuSiC (Dehouck et al. 2011)、DUET (Pires et al. 2014a)、SDM (Worth et al. 2011)、mCSM (Pires et al. 2014b)、Dynamut2 (Rodrigues et al. 2021)、I-Mutant3-3D (Capriotti et al. 2005)、Rosetta (Kellogg et al. 2011) 和 FoldX (Schymkowitz et al. 2005)。结果列于表 2 中。
<img src="https://cloudflare.remsait.com/img/ddgemb202509221824082.png"  alt="404" title=""  />
<center><div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      表 2
  	</div>
</center>
  对于每种方法，我们报告了以下三种情况下的皮尔逊相关系数（PCC）、均方根误差（RMSE）和平均绝对误差（MAE）：所有变异（包括正向和反向变异）的结果（“Total”列下）；仅正向变异的结果（“Direct”列下）；仅反向变异的结果（“Reverse”列下）。
  
  此外，我们还计算了反向相关性$(PCC_{d-r})$和反称性偏差$<\delta>$。
  
  在 S669 数据集上，DDGemb 在 PCC、RMSE 和 MAE 指标上均取得了最高性能（在“Total”、“Direct”和“Reverse”三类中均表现最佳）。总体而言，DDGemb 是本基准测试中性能最优的工具，其表现显著超过了现有的基于结构和基于序列的方法。
### 多点突变的预测
  最后，我们使用 PTmul-NR 数据集测试了 DDGemb 在多点突变预测上的性能。这使得我们能够直接与 DDGun/DDGun3D (Montanucci et al. 2019)、MAESTRO (Laimer et al. 2015) 和 FoldX (Schymkowitz et al. 2005) 等方法进行比较。结果列于表 3 中。
  
  在 PTmul-NR 数据集上，DDGemb 在多点突变预测方面显著优于 DDGun、DDGun3D、FoldX 和 MAESTRO，取得了最高的 0.59 相关系数，以及最低的 RMSE（2.16）和 MAE（1.59）值。这些结果表明，DDGemb 可以高精度地有效评估多点突变对蛋白质稳定性的影响。值得注意的是，我们的模型仅使用单点突变数据进行训练，却能在多点突变上取得优异性能，这表明该方法具备良好的泛化能力，能够推广到多点突变场景。
## 讨论
  在本研究中，我们提出了 DDGemb，一种基于蛋白质语言模型和 Transformer 架构的新型方法，用于预测单点和多点突变引起的蛋白质稳定性变化（ΔΔG）。该方法在一个从文献中整理的高质量数据集上进行训练，并使用近年来新发布的单点和多点突变热力学数据基准集进行测试。在所有基准测试中，DDGemb 的性能均优于当前最先进的方法，不仅超越了基于序列的方法，也优于基于结构的方法，在单点突变预测上取得了 0.68 的总体皮尔逊相关系数（PCC）。此外，在多点突变预测上，本方法的 PCC 达到 0.59，显著高于排名第二的最佳方法 FoldX（PCC 为 0.36）。本研究表明，将专为预测突变 ΔΔG 而精细调优的 Transformer 架构，与蛋白质语言模型提供的数值化表示相结合，具有重要意义。
  
  利益冲突：无。






