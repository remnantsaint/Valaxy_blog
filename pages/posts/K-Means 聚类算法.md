---
layout: post
title: K-Means 聚类算法
date: 2024-09-23 11:44:10
cover: 
top: 
tags: 机器学习
categories: 
  - 人工智能
  - 机器学习
# author: @Remsait
---
## 聚类
### 概念
&emsp; 聚类就是将一个具有相似特征的数据集合自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好。  
&emsp; 聚类是一种无监督学习方法，不需要预先标注好的训练集。聚类与分类最大的区别就是分类的目标事先已知，例如数字识别，在分类前就已经知道要将它分为哪几个数字，而聚类之前，目标是未知的，只能通过按照特征自动分为几类，然后认为给出每一类的定义（即簇识别）。  
&emsp; 至于“相似”这一概念，是利用距离这个评价标准来衡量的，我们通过计算对象之间的距离远近来判断他们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中欧式距离是最为简单常用的，除此之外还有曼哈顿距离和余弦相似性距离等。  
&emsp; 欧氏距离就是两点距离公式：  
$$d(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ··· + (x_n-y_n)^2} = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2 }$$

## K-Means 算法
&emsp; K-Means 是发现给定数据集的 K 个簇的聚类算法，之所以称之为 `K-均值` 是因为它可以发现 K 个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。  
&emsp; 每个簇 K 是用户指定的，每一个簇通过其质心，即簇中所有点的中心描述。  
&emsp; 聚类与分类算法的最大区别在于：费雷的目标类别已知，而聚类的目标类别是未知的。  
- 优点：
	1. 属于无监督学习，无需准备训练集
	2. 原理简单，实现起来较为容易
	3. 结果可解释性较好
- 缺点：
	1. 需手动设置 K 值，
	2. 可能收敛到局部最小值，在大规模数据集上收敛较慢
	3. 对于异常点、离群点敏感
- 使用数据类型：数值型数据

### K-Means 场景
&emsp; 用于数据集内种类属性不明晰，希望通过数据挖掘出或自动归类出相似特点的对象的场景。

### K-Means 术语
- 簇：所有数据的点集合，簇中的对象是 相似 的。
- 质心：簇中所有点的中心（计算所有点的均值而来）
- SSE： Sum of Sqared Error（误差平方和），用来评估模型的好坏，SSE 值越小，表示越接近它们的质心，聚类效果越好。由于对误差取了平方，因此更加注重那些远离中心的点。

### K-Means 工作流程
1. 随机确定 K 个初始点作为质心（不必是数据中的点）
2. 将数据集中的每个点分配到一个簇中，具体来讲就是为每个点找到距其最近的质心，并将其分配该质心所对应的簇，这一步完成之后，每个簇的质心更新为该簇所有点的平均值。
3. 重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。

伪代码如下：  
```text
创建 k 个点作为起始质心（通常是随机选择）
当任意一个点的簇分配结果发生改变时（不改变时算法结束）
	对数据集中的每个数据点
		对每个质心
			计算质心与数据点之间的距离
		将数据点分配到距其最近的簇
	对每一个簇, 计算簇中所有点的均值并将均值作为质心
```

### K-Means 开发流程
```text
收集数据: 使用任意方法
准备数据: 需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算
分析数据: 使用任意方法
训练算法: 不适用于无监督学习，即无监督学习不需要训练步骤
测试算法: 应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.
使用算法: 可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.
```

### K-Means 聚类算法的缺陷
> 在K-Means 的函数测试中，可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）

&emsp; 出现这个问题有很多原因，可能是 K 值取得不合适，也可能是距离函数不合适，也可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。  

&emsp; 为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大 SSE 值的簇划分为两个簇，具体实现可以将最大簇包含的点过滤出来并在这些点上运行 K-均值算法，令 K 设置为2.  

&emsp; 为了保持簇总数不变，可以将某两个簇进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？  

&emsp; 有两种可以量化的办法: 合并最近的质心，或者合并两个使得SSE增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。  

&emsp; 因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为二分K-均值（bisecting K-Means）的算法.

## 二分 K-Means 聚类算法
&emsp; 该算法首先将所有点作为一个簇，然后将该簇一分为二。  

&emsp; 之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE 的值。  

&emsp; 上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。

### 二分 K-Means 聚类算法伪代码
```text
将所有点看成一个簇
当簇数目小于 k 时
对于每一个簇
	计算总误差
	在给定的簇上面进行 KMeans 聚类（k=2）
	计算将该簇一分为二之后的总误差
选择使得误差最小的那个簇进行划分操作
```











## Reference








