---
layout: post
title: 预测数值型数据：回归
date: 2024-09-19 09:48:55
cover: 
top: 
tags: 机器学习
categories: 
  - 人工智能
  - 机器学习
# author: @Remsait
---
## 回归 概述
前面集中分类的目标变量是标称型数据，而回归是对连续型数据作出处理，回归的目的是预测数值型数据的目标值

## 回归 场景
回归的目的是预测数值型的目标值，最直接的办法是依据输入写出一个目标值的计算公式  
  
也就是说写出一个 回归方程 ，就要得到回归系数来做预测。  
 
回归一般指线性回归，线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。

## 回归 原理
### 线性回归
#### 矩阵求逆
1. 我们在计算回归方程的回归系数时，要用到以下公式：$\omega = (X^TX)^{-1}X^Ty$     
2. 需要对矩阵求逆，因此这个方程只在逆矩阵存在时适用，我们在程序代码中对此

#### 最小二乘法
- 最小二乘法是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配

#### 线性回归 工作原理
- 读入数据，将数据特征x、特征标签y存储在矩阵x、y中
- 验证 x^Tx 矩阵是否可逆
- 使用最小二乘法求得 回归系数 w 的最佳估计

#### 线性回归 开发流程
```text
收集数据: 采用任意方法收集数据
准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据
分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
训练算法: 找到回归系数
测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果
使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签
```

#### 线性回归 算法特点
- 优点: 结果易于理解，计算上不复杂。
- 缺点: 对非线性的数据拟合不好。
- 适用于数据类型: 数值型和标称型数据。

### 局部加权线性回归
#### 局部加权线性回归概述
1. 线性回归的以恶搞问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。
2. 一个方法是局部加权线性回归，在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归，我们需要最小化的目标函数大致为：$\sum_{i}^{} \omega (y^{(i)} - {\hat y}^{(i)})^2$
3. 目标函数中 w 为权重，不是回归系数。与 KNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下：$\hat w = (X^TWX)^{-1}X^TWy$   。其中 W 是一个矩阵，用来给每个数据点赋予权重，$\hat w$则为回归系数，这两个是不同概念
4. LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下:$w(i) = \exp \left( \frac{(x^{(i)} - x)^2}{-2k^2} \right)$ 
5. 这样就构建了一个只含对角元素的权重矩阵 w，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数。

#### 局部加权线性回归 工作原理
- 读入数据，将数据特征x、特征标签y存储在矩阵x、y中
- 利用高斯核构造一个权重矩阵 W，对预测点附近的点施加权重
- 验证 X^TWX 矩阵是否可逆
- 使用最小二乘法求得 回归系数 w 的最佳估计

















