---
categories: 
  - 人工智能
  - 机器学习
cover: 
date: 2024-09-06 09:34:09
image: 
layout: post
tags: 
  - 机器学习
time_warning: true
title: 决策树
top: 10
---
## 前言
&emsp; 经过研一第一个月的学习汇报，导师让我选择一个机器学习算法和生物信息数据库弄懂，并且在一个月后的组会上汇报，借此我选择了决策树这个算法，完善这篇文章。
<!-- more -->

## 决策树概述
1. 决策树算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一，现在只讨论用于分类的决策树
2. 决策树呈树形结构，依据特征对实例进行分类，可认为是`if-then`的集合
	- 由决策树的根节点到叶结点的每一条路径构建一条规则
	- 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论
	- 决策树的路径或其对应的`if-then`规则集合具有一个重要性质：互斥且完备
3. 通常包括三个步骤：特征选择、决策树的生成、决策树的修剪

经典游戏：“二十个问题”

## 决策树模型与学习
### 决策树模型
1. 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点 (internal node) 和叶结点 (leaf node) 。内部结点表示一个特征或属性 (features) ，叶结点表示一个类 (labels) 。
2. 分类过程：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点。最后将实例分配到叶结点的类中。

### 决策树学习
> 决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类

&emsp; 决策树本质上是从训练数据集中归纳出一组分类规则，我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，有无数个基于特征空间划分的类的条件概率模型，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。

## 决策树原理
### 信息熵&信息增益
1. 熵（entropy）：熵指的是体系的混乱程度，在不同学科中也有引申出更为具体的定义，是各领域十分重要的参量
2. 信息论中的熵（information theory）：是一种信息的度量方式，表示信息的混乱程度，也就是说信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。
3. 信息增益（information gain）：在划分数据集前后信息发生的变化称为信息增益

### 工作原理
构建一个决策树，可以用以下 createBranch() 方法：
```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

### 开发流程
```
收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

### 算法特点
```
优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征，模型具有可读性，分类速度快。
缺点: 容易过拟合。
适用数据类型: 数值型和标称型。
```

## 参考链接
<https://github.com/remnantsaint/ailearning/blob/master/docs/ml/3.md>