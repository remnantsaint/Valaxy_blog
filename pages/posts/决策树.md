---
categories: 
  - 人工智能
  - 机器学习
cover: 
date: 2024-09-06 09:34:09
image: 
layout: post
tags: 
  - 机器学习
time_warning: true
title: 决策树
top: 10
---

## 前言
&emsp; 经过研一第一个月的学习汇报，导师让我选择一个机器学习算法和生物信息数据库弄懂，并且在一个月后的组会上汇报，借此我选择了决策树这个算法，完善这篇文章。

<!-- more -->

## 决策树概述
1. 决策树算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一，现在只讨论用于分类的决策树
2. 决策树呈树形结构，依据特征对实例进行分类，可认为是`if-then`的集合
	- 由决策树的根节点到叶结点的每一条路径构建一条规则
	- 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论
	- 决策树的路径或其对应的`if-then`规则集合具有一个重要性质：互斥且完备
3. 通常包括三个步骤：特征选择、决策树的生成、决策树的修剪

经典游戏：“二十个问题”

## 决策树模型与学习
### 决策树模型
1. 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点 (internal node) 和叶结点 (leaf node) 。内部结点表示一个特征或属性 (features) ，叶结点表示一个类 (labels) 。
2. 分类过程：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点。最后将实例分配到叶结点的类中。

&emsp; 决策树的结构如下图所示  
![](https://cloudflare.remsait.com/img/%E5%86%B3%E7%AD%96%E6%A0%911.png)

### 决策树学习
> 决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类

&emsp; 决策树本质上是从训练数据集中归纳出一组分类规则，我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，有无数个基于特征空间划分的类的条件概率模型，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。  

&emsp; 决策树学习用损失函数表示这一目标，通常使用的是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题，因为从所有可能的决策树中选取最优是 NP 完全问题，所以现实中采用启发式方法，近似求解这一最优化问题，得到的决策树是次最优的。  

&emsp; 决策树学习的算法通常是以恶搞递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这个过程对应着对特征空间的划分，也对应着决策树的构建。  

&emsp; 开始，构建根节点，将所有训练数据都放在根结点，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中区；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类，这就生成了一颗决策树。  

&emsp; 以上方法可能对训练数据有很好的分类能力，但是对未知的测试数据未必可以，可能过拟合，所以需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体来说就是去掉过于细分的叶结点，使其回退到父节点，甚至更高的结点，然后将父节点或更高的结点改为新的叶结点。  

&emsp; 可以看出决策树学习算法包含特征选择、决策树的生成、决策树的剪枝，由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型，决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择，决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。  

&emsp; 整体流程图如下图所示：
![](https://cloudflare.remsait.com/img/%E5%86%B3%E7%AD%96%E6%A0%912.png)

## 决策树原理
### 特征选择
&emsp; 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征分类于随机分类的结果没有很大差别，则称这个特征是没有分类能力的，通常特征选择的准则是 信息增益 或 信息增益比。  

&emsp; 究竟选择哪个特征来分类更好些？直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。
### 信息熵&信息增益
1. 熵（entropy）：熵指的是体系的混乱程度，在不同学科中也有引申出更为具体的定义，是各领域十分重要的参量
2. 信息论中的熵（information theory）：是一种信息的度量方式，表示信息的混乱程度（随机变量不确定性的度量），也就是说信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。 
 
&emsp; 假设某随机变量的概率分布为：
$$P(X = x_i) = p_i , i = 1, 2,......., n$$  
&emsp; 则它的信息熵计算公式为：
$$H(X) = - \sum_{i = 1}^{n}p_ilogp_i$$
&emsp; 条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性，公式如下：
$$H(Y|X) = \sum_{i = 1}^{n}p_iH(Y|X = x_i)$$

3. 信息增益（information gain）：表示得知特征 X 的信息而使类 Y 的信息的不确定性减少的程度。  

&emsp; 定义：特征 A 对训练数据集 D 的信息增益 g(D, A) ，定义为集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差，即： 
$$g(D,A) = H(D) - H(D|A)$$
&emsp; 不同特征往往拥有不同的信息增益，信息增益大的特征具有更强的分类能力。

&emsp; 假设训练数据集为 D，|D| 为样本容量（总人数），有 K 个类$C_k$（是否批准贷款申请），$|C_k|$ 为属于类 $C_k$ 的样本个数；设特征 A 有 n 个不同的取值 {a1,a2,...an} （年龄中的青年老年中年等），根据特征 A 的取值将 D 划分为 n 个子集 $D_1,D_2,....,D_n$ （青年老年中年的人数）；记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D\text{}_{ik}$ （例如青年中有多少人批准贷款申请）  
![](https://cloudflare.remsait.com/img/%E5%86%B3%E7%AD%96%E6%A0%913.png)

&emsp; 信息增益算法如下：  
&emsp; &emsp; (1) 计算数据集 D 的经验熵 H(D) 
$$H(D) = - \sum_{k = 1}^{K}\frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}$$
&emsp; &emsp; (2) 计算特征 A 对数据集 D 的经验条件熵 H(D|A) 
$$H(D|A) = \sum_{i = 1}^{n}\frac{|D_i|}{|D|}H(D_i) = - \sum_{i = 1}^{n}\frac{|D_i|}{|D|}\sum_{k = 1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$
&emsp;&emsp;  (3) 计算信息增益
$$g(D,A) = H(D) - H(D|A)$$

4. 信息增益比  
&emsp; 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行矫正，这是特征选择的另一准则
&emsp; 特征 A 对训练数据集 D 的信息增益比 $g_R(D,A)$ 定义为其信息增益 g(D,A) 与训练数据集 D 关于特征 A 的值的熵 $H_A(D)$ 之比，即：
$$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$$
&emsp; 其中
$$H_A(D) = - \sum_{i =1}^{n}\frac{D_i}{D}logs_2\frac{D_i}{D}$$

### 决策树的生成

### 决策树的剪枝

### 工作原理
构建一个决策树，可以用以下 createBranch() 方法：
```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

### 开发流程
```
收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

### 算法特点
```
优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征，模型具有可读性，分类速度快。
缺点: 容易过拟合。
适用数据类型: 数值型和标称型。
```

## 参考链接
&emsp; <https://github.com/remnantsaint/ailearning/blob/master/docs/ml/3.md>  
&emsp; <https://blog.csdn.net/GreenYang5277/article/details/104500739#:~:text=%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%80%9D%E6%83%B3%E4%B8%BB%E8%A6%81%E6%9D%A5%E6%BA%90#:~:text=%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%80%9D%E6%83%B3%E4%B8%BB%E8%A6%81%E6%9D%A5%E6%BA%90>  
&emsp; [统计学习方法](https://zh.z-lib.gs/book/18259885/98f785/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%AC2%E7%89%88.html?dsource=recommend)