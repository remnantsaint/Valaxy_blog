---
categories: 
  - 人工智能
  - 机器学习
cover: 
date: 2024-09-06 09:34:09
image: 
layout: post
tags: 
  - 机器学习
time_warning: true
title: 决策树
top: 10
---
## 前言
&emsp; 经过研一第一个月的学习汇报，导师让我选择一个机器学习算法和生物信息数据库弄懂，并且在一个月后的组会上汇报，借此我选择了决策树这个算法，完善这篇文章。
<!-- more -->

## 决策树概述
1. 决策树算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一，现在只讨论用于分类的决策树
2. 决策树呈树形结构，依据特征对实例进行分类，可认为是`if-then`的集合
	- 由决策树的根节点到叶结点的每一条路径构建一条规则
	- 路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论
	- 决策树的路径或其对应的`if-then`规则集合具有一个重要性质：互斥且完备
3. 通常包括三个步骤：特征选择、决策树的生成、决策树的修剪

经典游戏：“二十个问题”

## 决策树模型与学习
### 决策树模型
1. 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点 (internal node) 和叶结点 (leaf node) 。内部结点表示一个特征或属性 (features) ，叶结点表示一个类 (labels) 。
2. 分类过程：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点。最后将实例分配到叶结点的类中。

### 决策树学习
> 决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类

&emsp; 决策树本质上是从训练数据集中归纳出一组分类规则，我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，有无数个基于特征空间划分的类的条件概率模型，我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。  

&emsp; 决策树学习用损失函数表示这一目标，通常使用的是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题，因为从所有可能的决策树中选取最优是 NP 完全问题，所以现实中采用启发式方法，近似求解这一最优化问题，得到的决策树是次最优的。  

&emsp; 决策树学习的算法通常是以恶搞递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这个过程对应着对特征空间的划分，也对应着决策树的构建。  

&emsp; 开始，构建根节点，将所有训练数据都放在根结点，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中区；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类，这就生成了一颗决策树。  

&emsp; 以上方法可能对训练数据有很好的分类能力，但是对未知的测试数据未必可以，可能过拟合，所以需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体来说就是去掉过于细分的叶结点，使其回退到父节点，甚至更高的结点，然后将父节点或更高的结点改为新的叶结点。  

&emsp; 可以看出决策树学习算法包含特征选择、决策树的生成、决策树的剪枝，由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型，决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择，决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。

## 决策树原理
### 信息熵&信息增益
1. 熵（entropy）：熵指的是体系的混乱程度，在不同学科中也有引申出更为具体的定义，是各领域十分重要的参量
2. 信息论中的熵（information theory）：是一种信息的度量方式，表示信息的混乱程度，也就是说信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。
3. 信息增益（information gain）：在划分数据集前后信息发生的变化称为信息增益

### 工作原理
构建一个决策树，可以用以下 createBranch() 方法：
```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

### 开发流程
```
收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

### 算法特点
```
优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征，模型具有可读性，分类速度快。
缺点: 容易过拟合。
适用数据类型: 数值型和标称型。
```

## 参考链接
<https://github.com/remnantsaint/ailearning/blob/master/docs/ml/3.md>