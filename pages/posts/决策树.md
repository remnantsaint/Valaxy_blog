---
categories: 人工智能
cover: 
date: 2024-09-06 09:34:09
image: 
layout: post
tags: 
  - 机器学习
time_warning: true
title: 决策树
top: 
---

#### 决策树概述
1. 决策树算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一，现在只讨论用于分类的决策树
2. 决策树呈树形结构，依据特征对实例进行分类，可认为是`if-then`的集合
3. 通常包括三个步骤：特征选择、决策树的生成、决策树的修剪

经典游戏：“二十个问题”

#### 决策树定义
1. 分类决策树模型是描述对实例进行分类的树形结构。决策树由点和有向边组成。结点有两种类型：内部结点 (internal node) 和叶结点 (leaf node) 。内部结点表示一个特征或属性 (features) ，叶结点表示一个类 (labels) 。
2. 分类过程：从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点。最后将实例分配到叶结点的类中。

#### 决策树原理
##### 信息熵&信息增益
1. 熵（entropy）：熵指的是体系的混乱程度，在不同学科中也有引申出更为具体的定义，是各领域十分重要的参量
2. 信息论中的熵（information theory）：是一种信息的度量方式，表示信息的混乱程度，也就是说信息越有序，信息熵越低。例如：火柴有序放在火柴盒里，熵值很低，相反，熵值很高。
3. 信息增益（information gain）：在划分数据集前后信息发生的变化称为信息增益

##### 工作原理
构建一个决策树，可以用以下 createBranch() 方法：
```python
def createBranch():
'''
此处运用了迭代的思想。 感兴趣可以搜索 迭代 recursion， 甚至是 dynamic programing。
'''
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```

##### 开发流程
```
收集数据: 可以使用任何方法。
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART)
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
训练算法: 构造树的数据结构。
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。
```

##### 算法特点
```
优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。
缺点: 容易过拟合。
适用数据类型: 数值型和标称型。
```

#### 参考链接
<https://github.com/remnantsaint/ailearning/blob/master/docs/ml/3.md>