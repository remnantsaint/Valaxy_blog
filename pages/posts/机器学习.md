---
categories: 人工智能
cover: 
date: 2023-12-28 12:48:27
image: 
layout: post
tags: 
time_warning: true
title: 机器学习
top: 
---

### 第一周

#### 引言

监督学习：给出的数据集中每个样本都有对应的“正确答案”
输出0、1、2这种离散值的是分类问题；输出预测的一个准确值值的是回归问题

无监督学习：交给算法大量的无标签数据，并让算法为我们从数据中找出某种结构
例如新闻分类、细分市场

#### 单变量线性回归
线性函数：$h_{\theta}(x) = \theta_0 + \theta_1 x_1$
线性回归问题代价函数：$J(\theta_0,\theta_1) = \frac{1} {2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$ ，需要一种有效的算法，自动地找出使该函数取最小值的参数$\theta_0$和$\theta_1$

用梯度下降算法最小化任意函数J()
思想：开始时随机选择一个参数的组合，计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合，持续这么做得到一个局部最小值；选择不同的初始参数组合，找到不同的局部最小值。

梯度下降算法公式：
repeat until convergence {}
&emsp; $\theta_j := \theta_j - \alpha \frac{\partial J(\theta_0,\theta_1)}{\partial \theta_j}$  (for j = 0 and j = 1)
}
反复更新$\theta_0$和$\theta_1$等
实现细节是先计算 := 后面的，再给各个$\theta_j$赋值，即同步更新
其中$\alpha$是学习率，决定我们沿着能让代价函数下降程度最大的方向向下迈的步子有多大。

求偏导即是求在某点切线的斜率，斜率越小，越接近局部最小值，移动的步幅越小
若$\alpha$太小，可能需要很多步才到最低点；若$\alpha$太大，会导致无法收敛，甚至发散。

##### 线性回归算法
将线性回归算法和梯度下降算法结合，得到批量梯度下降算法（通常不命名）
Repeat {
&emsp; $\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})$
&emsp; $\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)})$
}
即用 $\theta_j-\alpha\cdot [J()$对$\theta_j$求偏导$]$
也可以用线性代数的**正规方程**方法来计算，但不如梯度下降算法简单。

掌握线性代数中矩阵的基本知识

### 第二周
#### 多变量线性回归
##### 多维特征
当有多个特征，设$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n$
公式中有n+1个参数和n个变量，为了将公式简化，设$x_0=1$，则公式转化为$h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n$
此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m*(n+1)。
因此公式可以简化为$h_\theta(x) = \theta^T X$，其中$\theta$和X都是列向量，该公式即多元线性回归

##### 多变量梯度下降
与单变量线性回归类似，也有代价函数：$J(\theta_0,\theta_1...\theta_n) = \frac{1} {2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$
批量梯度下降算法也一样，不过从$\theta_0$和$\theta_1$上升到$\theta_n$
求导前：$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2$
求导后公式为：$\theta_n := \theta_n - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta(x^{(i)}) - y^{(i)}) \cdot x_n^{(i)})$        只有n变化
计算代价函数代码如下：
```python
def computeCost(x,y,theta):
	inner = np.power(((x * theta.T) - y), 2)
	return np.sum(inner) / (2 * len(x))
```

##### 梯度下降法实践 1-特征缩放
在我们面对多维特征问题的时候，要保证这些特征都具有相近的尺度，否则画出等高线图时会发现图像很扁，梯度下降算法需要非常多次迭代才能收敛。
解决方法是尝试将所有特征的尺度都尽量缩放到-1到1之间，最简单的方法是：
$x_n = \frac{x_n - \mu_n}{s_n}$
其中$\mu_n$是平均值，$s_n$是标准差

##### 梯度下降法实践 2-学习率
学习率过大，就可能导致无法收敛，若过小，又会导致移动过慢
所以考虑取下列学习率：$\alpha = 0.001 , 0.003 , 0.01 , 0.03 , 0.1 , 0.3 , 1 , 3......$以3为倍数

##### 特征和多项式回归
我们可以创造新的特征值，利用特征值之间的联系，合并，减少特征值的数量。
线性回归并不适用所有数据，有时需要用曲线来使用数据，如三次方模型$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2^2 + \theta_3 x_3^3$，可用$x_2$代替$x_2^2$，$x_3$代替$x_3^3$，从而将模型转化为线性回归模型
注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征放缩是非常有必要的，因为包含高阶

##### 正规方程
公式：$\theta = (X^T X)^{-1} X^T y$
适用于特征变量小于一万、线性回归问题
python代码如下：

```python
import numpy as np
# 添加偏置项 x0 = 1
X_bias = np.c_[np.ones((X.shape[0], 1)), X]
# 计算正规方程
theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y
```



&emsp; 参考：
* 吴恩达机器学习
* [黄海广机器学习笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)