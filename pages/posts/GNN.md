---
layout: post
title: GNN
date: 2025-09-12 14:12:10
updated: 2025-09-12
time_warning: true 
cover: 
top: 
tags: 
 - GNN
 - 深度学习
categories: 
 - 深度学习
# author: @Remsait
---
## 图是什么
  图是表示实体之间的一些关系，实体就是 nodes 点 ，关系就是 edges 边  

  图一般有两种，无向图 与 有向图

### 数据如何表示成图
  比如一张突变，长宽都是 244 ，通道是 3（244×244×3），我们一般会把它表示成一个 tensor 输入卷积神经网络。另外一个角度来说，可以把它当作一个图，它的每一个像素就是一个点，像素之间相连接关系就是边  

  其中要用到邻接矩阵，邻接矩阵的每一个行和列都表示一个顶点，如果第 i 行和第 j 列相交的点为 1 的话，代表这两个点相连（之间有边）  

  文本也是一样，把每个词当作一个点，如果词与词之间相连，就表示有边。分子图也是一样，一个原子表示一个点，原子之间相连表示边  

### 在图上面定义什么样的问题
  主要有三大类问题：图层面、顶点层面、边层面  

  图层面的问题可以是找图上有多少个环

  顶点层面的任务可以是判断点和点之间的关系程度

  边层面的任务可以是研究点和点之间边的属性

### 将神经网络用在图上面的挑战
  怎么样表示图，能让它和神经网络兼容。  

  图上面有四种信息（点、边、全局、连接性），前三者都能用向量表示，连接性可以用邻接矩阵（稀疏矩阵不行；顶点排序改变，邻接矩阵改变）邻接列表（邻接列表中第 i 个表示第 i 条边连接哪两个顶点，顺序无关


## 图神经网络
  GNN 是一个对图上所有的属性，包括顶点、边、全局上下文，进行一个可以优化的变换，这个变换能够保持住图的对称信息（把一个顶点打乱排序后结果不变）。GNN 的输入是一个图，输出也是一个图，它会对那些属性进行变化，但是不会改变这个图的连接性  

### 最简单的 GNN
  对一个图的顶点向量、边向量、全局向量分别构造一个 MLP，这三个 MLP 就组成一个 GNN 的层，输入和输出都是一个图，属性被更新过，但是整个图的结构没有变化

  如果想对一个顶点做预测，所有的点共享一个全连接层来输出。

  如果一个点没有自己的向量，还是想预测到它的向量，需要用到 polling (汇聚)，可以把跟这个点连接的边的向量和全局的向量拿出来，把这些向量加起来就能代表原来的点，做一个最后的全连接层得到输出（不同点的汇聚不一样，所以可以代表）

  对于全局的向量同理，把图中所有顶点向量加起来做一个汇聚，最后送入全局的全连接层  

  总结：输入一个图，进入一系列的 GNN 层，每层有三个分别对点、边、全局的 MLP，输出得到一个结构不边、属性改变的图。如果要对哪一个属性进行预测，就添加合适的输出层，缺失信息的话就加入合适的汇聚层，最后得到预测值

  局限性：没有考虑边、顶点、全局之间的连接性，结果不能充分利用图的信息  

### 改进最简单的 GNN
  用到信息传递

  原来的做法是把向量直接送入对应的 MLP 层，然后直接输出成更新的向量，但是在最简单信息传递中还要做额外的事

  需要把它的向量，和它邻居的两个的顶点的向量都加在一起得到一个汇聚的向量，汇聚的向量再进入 MLP ，这样邻居之间信息就能汇聚，完整了整个图的比较长距离的一个信息的传递

  可以在进入 MLP 层前把顶点的信息加到边，并把边的信息加到顶点，完成顶点到边和边到顶点的信息传递后再进入 MLP 做更新

  但是先把顶点信息加入边还是先把边信息加入顶点导致的结果不同，可以交替更新，把顶点信息传过去后先不加，先把边信息传到顶点信息之后一起加  

### 为什么要有全局的信息
  假设一个点，其与所有的边和点都相连，就是一个 U 

  U跟所有的 V 和 E 都相连，所以想把顶点信息和边信息互相汇聚时，也会把 U 的信息汇聚过来，并且之后也会把 E 和 V 的信息汇聚到 U，之后再做更新



  



















