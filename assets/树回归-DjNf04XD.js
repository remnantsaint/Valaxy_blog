import{_ as d}from"./ValaxyMain.vue_vue_type_style_index_0_lang-Lj6vZGyG.js";import{u as m,c as p,o as f,w as a,r as t,g as l,h as e,f as b,p as u}from"./app-D6Hejker.js";import"./YunFooter-BsWuc4uy.js";import"./YunCard.vue_vue_type_script_setup_true_lang-DrznGDEn.js";import"./index-C5okkQwF.js";import"./YunPageHeader.vue_vue_type_script_setup_true_lang-Dma2ZTu7.js";import"./post-C0fFnzag.js";const y={__name:"树回归",setup(g,{expose:c}){const i=JSON.parse('{"title":"树回归","description":"","frontmatter":{"layout":"post","title":"树回归","date":"2024-09-20 09:58:49","cover":null,"top":null,"tags":"机器学习","categories":["人工智能","机器学习"]},"headers":[{"level":2,"title":"树回归 概述","slug":"树回归-概述","link":"#树回归-概述","children":[{"level":3,"title":"树回归 场景","slug":"树回归-场景","link":"#树回归-场景","children":[]}]},{"level":2,"title":"树回归 原理","slug":"树回归-原理","link":"#树回归-原理","children":[{"level":3,"title":"树回归 原理概述","slug":"树回归-原理概述","link":"#树回归-原理概述","children":[]},{"level":3,"title":"树构建算法 比较","slug":"树构建算法-比较","link":"#树构建算法-比较","children":[]},{"level":3,"title":"树回归 工作原理","slug":"树回归-工作原理","link":"#树回归-工作原理","children":[]},{"level":3,"title":"树回归 开发流程","slug":"树回归-开发流程","link":"#树回归-开发流程","children":[]},{"level":3,"title":"树回归 算法特点","slug":"树回归-算法特点","link":"#树回归-算法特点","children":[]}]},{"level":2,"title":"树剪枝","slug":"树剪枝","link":"#树剪枝","children":[{"level":3,"title":"预剪枝","slug":"预剪枝","link":"#预剪枝","children":[]},{"level":3,"title":"后剪枝","slug":"后剪枝","link":"#后剪枝","children":[]}]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"relativePath":"pages/posts/树回归.md","path":"/home/runner/work/remnantsaint.github.io/remnantsaint.github.io/pages/posts/树回归.md","lastUpdated":1758348498000}'),r=m(),s=i.frontmatter||{};return r.meta.frontmatter=Object.assign(r.meta.frontmatter||{},i.frontmatter||{}),u("pageData",i),u("valaxy:frontmatter",s),globalThis.$frontmatter=s,c({frontmatter:{layout:"post",title:"树回归",date:"2024-09-20 09:58:49",cover:null,top:null,tags:"机器学习",categories:["人工智能","机器学习"]}}),(n,o)=>{const h=d;return f(),p(h,{frontmatter:b(s)},{"main-content-md":a(()=>[...o[0]||(o[0]=[l("h2",{id:"树回归-概述",tabindex:"-1"},[e("树回归 概述 "),l("a",{class:"header-anchor",href:"#树回归-概述","aria-label":'Permalink to "树回归 概述"'},"​")],-1),l("ul",null,[l("li",null,"树回归可以用于分类，也可以用于回归")],-1),l("h3",{id:"树回归-场景",tabindex:"-1"},[e("树回归 场景 "),l("a",{class:"header-anchor",href:"#树回归-场景","aria-label":'Permalink to "树回归 场景"'},"​")],-1),l("p",null,"  线性回归有一些强大的方法，到那时这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙，而且现实生活中很多问题都是非线性的，不能使用全局线性模型来拟合任何数据。",-1),l("p",null,"  一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。",-1),l("h2",{id:"树回归-原理",tabindex:"-1"},[e("树回归 原理 "),l("a",{class:"header-anchor",href:"#树回归-原理","aria-label":'Permalink to "树回归 原理"'},"​")],-1),l("h3",{id:"树回归-原理概述",tabindex:"-1"},[e("树回归 原理概述 "),l("a",{class:"header-anchor",href:"#树回归-原理概述","aria-label":'Permalink to "树回归 原理概述"'},"​")],-1),l("ul",null,[l("li",null,"为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度"),l("li",null,"在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。"),l("li",null,"上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。")],-1),l("h3",{id:"树构建算法-比较",tabindex:"-1"},[e("树构建算法 比较 "),l("a",{class:"header-anchor",href:"#树构建算法-比较","aria-label":'Permalink to "树构建算法 比较"'},"​")],-1),l("ul",null,[l("li",null,"在决策树中我们使用的构建算法是 ID3 ，ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。"),l("li",null,"除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。"),l("li",null,"CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第决策树中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。"),l("li",null,"回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。")],-1),l("h4",{id:"常见树构造算法的划分分支方式",tabindex:"-1"},[e("常见树构造算法的划分分支方式 "),l("a",{class:"header-anchor",href:"#常见树构造算法的划分分支方式","aria-label":'Permalink to "常见树构造算法的划分分支方式"'},"​")],-1),l("p",null,"  常用的是三个算法：ID3 、C4.5、CART，三种方法区别的是划分树的分支的方式",-1),l("ul",null,[l("li",null,"ID3 是信息增益分支"),l("li",null,"C4.5 是信息增益率分支"),l("li",null,"CART 做分类工作时，采用 CINI 值作为节点分类的依据；回归时，采用样本最小方差最为结点的分裂依据。")],-1),l("p",null,[e("  总的来说，CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；"),l("br"),e("   以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。")],-1),l("h3",{id:"树回归-工作原理",tabindex:"-1"},[e("树回归 工作原理 "),l("a",{class:"header-anchor",href:"#树回归-工作原理","aria-label":'Permalink to "树回归 工作原理"'},"​")],-1),l("ol",null,[l("li",null,"找到数据集切分的最佳位置，函数 chooseBestSplit()伪代码如下：")],-1),l("div",{style:{"max-height":"300px"},class:"language-text vp-adaptive-theme"},[l("button",{title:"Copy Code",class:"copy"}),l("span",{class:"lang"},"text"),l("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[l("code",{"v-pre":""},[l("span",{class:"line"},[l("span",null,"对每个特征:")]),e(`
`),l("span",{class:"line"},[l("span",null,"    对每个特征值: ")]),e(`
`),l("span",{class:"line"},[l("span",null,"        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）")]),e(`
`),l("span",{class:"line"},[l("span",null,"        计算切分的误差")]),e(`
`),l("span",{class:"line"},[l("span",null,"        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差")]),e(`
`),l("span",{class:"line"},[l("span",null,"返回最佳切分的特征和阈值")])])]),l("button",{class:"collapse"})],-1),l("ol",{start:"2"},[l("li",null,"树构建算法，函数 createTree() 伪代码如下")],-1),l("div",{style:{"max-height":"300px"},class:"language-text vp-adaptive-theme"},[l("button",{title:"Copy Code",class:"copy"}),l("span",{class:"lang"},"text"),l("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[l("code",{"v-pre":""},[l("span",{class:"line"},[l("span",null,"找到最佳的待切分特征:")]),e(`
`),l("span",{class:"line"},[l("span",null,"    如果该节点不能再分，将该节点存为叶节点")]),e(`
`),l("span",{class:"line"},[l("span",null,"    执行二元切分")]),e(`
`),l("span",{class:"line"},[l("span",null,"    在右子树调用 createTree() 方法")]),e(`
`),l("span",{class:"line"},[l("span",null,"    在左子树调用 createTree() 方法")])])]),l("button",{class:"collapse"})],-1),l("h3",{id:"树回归-开发流程",tabindex:"-1"},[e("树回归 开发流程 "),l("a",{class:"header-anchor",href:"#树回归-开发流程","aria-label":'Permalink to "树回归 开发流程"'},"​")],-1),l("ol",null,[l("li",null,"收集数据: 采用任意方法收集数据。"),l("li",null,"准备数据: 需要数值型数据，标称型数据应该映射成二值型数据。"),l("li",null,"绘出数据的二维可视化显示结果，以字典方式生成树。"),l("li",null,"大部分时间都花费在叶节点树模型的构建上。"),l("li",null,"测试算法: 使用测试数据上的R^2值来分析模型的效果。"),l("li",null,"使用算法: 使用训练处的树做预测，预测结果还可以用来做很多事情。")],-1),l("h3",{id:"树回归-算法特点",tabindex:"-1"},[e("树回归 算法特点 "),l("a",{class:"header-anchor",href:"#树回归-算法特点","aria-label":'Permalink to "树回归 算法特点"'},"​")],-1),l("ol",null,[l("li",null,"优点：可以对复杂和非线性的数据建模"),l("li",null,"缺点：结果不易理解"),l("li",null,"适用数据类型：数值型和标称型数据")],-1),l("h2",{id:"树剪枝",tabindex:"-1"},[e("树剪枝 "),l("a",{class:"header-anchor",href:"#树剪枝","aria-label":'Permalink to "树剪枝"'},"​")],-1),l("ul",null,[l("li",null,"一棵树如果节点过多，表明该模型可能出现了“过拟合”"),l("li",null,[e("通过降低决策树的复杂度来避免过拟合的过程称为 "),l("code",null,"剪枝"),e("。在函数 chooseBestSplit() 中提前终止条件，实际上是在进行一种所谓的"),l("code",null,"预剪枝（prepruning）"),e("操作。另一个形式的剪枝需要使用测试集和训练集，称作 "),l("code",null,"后剪枝（postpruning）"),e("。")])],-1),l("h3",{id:"预剪枝",tabindex:"-1"},[e("预剪枝 "),l("a",{class:"header-anchor",href:"#预剪枝","aria-label":'Permalink to "预剪枝"'},"​")],-1),l("ul",null,[l("li",null,"顾名思义，预剪枝就是及早的停止树增长，在构造决策树的同时进行剪枝。"),l("li",null,"所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。")],-1),l("h3",{id:"后剪枝",tabindex:"-1"},[e("后剪枝 "),l("a",{class:"header-anchor",href:"#后剪枝","aria-label":'Permalink to "后剪枝"'},"​")],-1),l("ul",null,[l("li",null,[e("决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。合并也被称作 "),l("code",null,"塌陷处理"),e(" ，在回归树中一般采用取需要合并的所有子树的平均值。后剪枝是目前最普遍的做法。")])],-1),l("h2",{id:"reference",tabindex:"-1"},[e("Reference "),l("a",{class:"header-anchor",href:"#reference","aria-label":'Permalink to "Reference"'},"​")],-1),l("p",null,[l("a",{href:"https://github.com/remnantsaint/ailearning/blob/master/docs/ml/9.md",target:"_blank",rel:"noreferrer"},"https://github.com/remnantsaint/ailearning/blob/master/docs/ml/9.md")],-1)])]),"main-header":a(()=>[t(n.$slots,"main-header")]),"main-header-after":a(()=>[t(n.$slots,"main-header-after")]),"main-nav":a(()=>[t(n.$slots,"main-nav")]),"main-content":a(()=>[t(n.$slots,"main-content")]),"main-content-after":a(()=>[t(n.$slots,"main-content-after")]),"main-nav-before":a(()=>[t(n.$slots,"main-nav-before")]),"main-nav-after":a(()=>[t(n.$slots,"main-nav-after")]),comment:a(()=>[t(n.$slots,"comment")]),footer:a(()=>[t(n.$slots,"footer")]),aside:a(()=>[t(n.$slots,"aside")]),"aside-custom":a(()=>[t(n.$slots,"aside-custom")]),default:a(()=>[t(n.$slots,"default")]),_:3},8,["frontmatter"])}}};export{y as default};
