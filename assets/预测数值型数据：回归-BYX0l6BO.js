import{_ as o}from"./ValaxyMain.vue_vue_type_style_index_0_lang-Lj6vZGyG.js";import{u as g,c as u,o as d,w as l,r as e,g as s,h as a,f as y,p}from"./app-D6Hejker.js";import"./YunFooter-BsWuc4uy.js";import"./YunCard.vue_vue_type_script_setup_true_lang-DrznGDEn.js";import"./index-C5okkQwF.js";import"./YunPageHeader.vue_vue_type_script_setup_true_lang-Dma2ZTu7.js";import"./post-C0fFnzag.js";const T={__name:"预测数值型数据：回归",setup(v,{expose:c}){const n=JSON.parse('{"title":"预测数值型数据：回归","description":"","frontmatter":{"layout":"post","title":"预测数值型数据：回归","date":"2024-09-19 09:48:55","cover":null,"top":null,"tags":"机器学习","categories":["人工智能","机器学习"]},"headers":[{"level":2,"title":"回归 概述","slug":"回归-概述","link":"#回归-概述","children":[]},{"level":2,"title":"回归 场景","slug":"回归-场景","link":"#回归-场景","children":[]},{"level":2,"title":"回归 原理","slug":"回归-原理","link":"#回归-原理","children":[{"level":3,"title":"线性回归","slug":"线性回归","link":"#线性回归","children":[]},{"level":3,"title":"局部加权线性回归","slug":"局部加权线性回归","link":"#局部加权线性回归","children":[]}]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"relativePath":"pages/posts/预测数值型数据：回归.md","path":"/home/runner/work/remnantsaint.github.io/remnantsaint.github.io/pages/posts/预测数值型数据：回归.md","lastUpdated":1758348498000}'),i=g(),m=n.frontmatter||{};return i.meta.frontmatter=Object.assign(i.meta.frontmatter||{},n.frontmatter||{}),p("pageData",n),p("valaxy:frontmatter",m),globalThis.$frontmatter=m,c({frontmatter:{layout:"post",title:"预测数值型数据：回归",date:"2024-09-19 09:48:55",cover:null,top:null,tags:"机器学习",categories:["人工智能","机器学习"]}}),(t,r)=>{const h=o;return d(),u(h,{frontmatter:y(m)},{"main-content-md":l(()=>[...r[0]||(r[0]=[s("h2",{id:"回归-概述",tabindex:"-1"},[a("回归 概述 "),s("a",{class:"header-anchor",href:"#回归-概述","aria-label":'Permalink to "回归 概述"'},"​")],-1),s("p",null,"前面集中分类的目标变量是标称型数据，而回归是对连续型数据作出处理，回归的目的是预测数值型数据的目标值",-1),s("h2",{id:"回归-场景",tabindex:"-1"},[a("回归 场景 "),s("a",{class:"header-anchor",href:"#回归-场景","aria-label":'Permalink to "回归 场景"'},"​")],-1),s("p",null,"回归的目的是预测数值型的目标值，最直接的办法是依据输入写出一个目标值的计算公式",-1),s("p",null,"也就是说写出一个 回归方程 ，就要得到回归系数来做预测。",-1),s("p",null,"回归一般指线性回归，线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出。",-1),s("h2",{id:"回归-原理",tabindex:"-1"},[a("回归 原理 "),s("a",{class:"header-anchor",href:"#回归-原理","aria-label":'Permalink to "回归 原理"'},"​")],-1),s("h3",{id:"线性回归",tabindex:"-1"},[a("线性回归 "),s("a",{class:"header-anchor",href:"#线性回归","aria-label":'Permalink to "线性回归"'},"​")],-1),s("h4",{id:"矩阵求逆",tabindex:"-1"},[a("矩阵求逆 "),s("a",{class:"header-anchor",href:"#矩阵求逆","aria-label":'Permalink to "矩阵求逆"'},"​")],-1),s("ol",null,[s("li",null,[a("我们在计算回归方程的回归系数时，要用到以下公式："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"ω"),s("mo",null,"="),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"X"),s("mi",null,"T")]),s("mi",null,"X"),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"1")])]),s("msup",null,[s("mi",null,"X"),s("mi",null,"T")]),s("mi",null,"y")]),s("annotation",{encoding:"application/x-tex"},"\\omega = (X^TX)^{-1}X^Ty")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"ω"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0913em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y")])])])]),s("li",null,"需要对矩阵求逆，因此这个方程只在逆矩阵存在时适用，我们在程序代码中对此")],-1),s("h4",{id:"最小二乘法",tabindex:"-1"},[a("最小二乘法 "),s("a",{class:"header-anchor",href:"#最小二乘法","aria-label":'Permalink to "最小二乘法"'},"​")],-1),s("ul",null,[s("li",null,"最小二乘法是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配")],-1),s("h4",{id:"线性回归-工作原理",tabindex:"-1"},[a("线性回归 工作原理 "),s("a",{class:"header-anchor",href:"#线性回归-工作原理","aria-label":'Permalink to "线性回归 工作原理"'},"​")],-1),s("ul",null,[s("li",null,"读入数据，将数据特征x、特征标签y存储在矩阵x、y中"),s("li",null,"验证 x^Tx 矩阵是否可逆"),s("li",null,"使用最小二乘法求得 回归系数 w 的最佳估计")],-1),s("h4",{id:"线性回归-开发流程",tabindex:"-1"},[a("线性回归 开发流程 "),s("a",{class:"header-anchor",href:"#线性回归-开发流程","aria-label":'Permalink to "线性回归 开发流程"'},"​")],-1),s("div",{style:{"max-height":"300px"},class:"language-text vp-adaptive-theme"},[s("button",{title:"Copy Code",class:"copy"}),s("span",{class:"lang"},"text"),s("pre",{class:"shiki shiki-themes github-light github-dark vp-code"},[s("code",{"v-pre":""},[s("span",{class:"line"},[s("span",null,"收集数据: 采用任意方法收集数据")]),a(`
`),s("span",{class:"line"},[s("span",null,"准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据")]),a(`
`),s("span",{class:"line"},[s("span",null,"分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比")]),a(`
`),s("span",{class:"line"},[s("span",null,"训练算法: 找到回归系数")]),a(`
`),s("span",{class:"line"},[s("span",null,"测试算法: 使用 R^2 或者预测值和数据的拟合度，来分析模型的效果")]),a(`
`),s("span",{class:"line"},[s("span",null,"使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签")])])]),s("button",{class:"collapse"})],-1),s("h4",{id:"线性回归-算法特点",tabindex:"-1"},[a("线性回归 算法特点 "),s("a",{class:"header-anchor",href:"#线性回归-算法特点","aria-label":'Permalink to "线性回归 算法特点"'},"​")],-1),s("ul",null,[s("li",null,"优点: 结果易于理解，计算上不复杂。"),s("li",null,"缺点: 对非线性的数据拟合不好。"),s("li",null,"适用于数据类型: 数值型和标称型数据。")],-1),s("h3",{id:"局部加权线性回归",tabindex:"-1"},[a("局部加权线性回归 "),s("a",{class:"header-anchor",href:"#局部加权线性回归","aria-label":'Permalink to "局部加权线性回归"'},"​")],-1),s("h4",{id:"局部加权线性回归概述",tabindex:"-1"},[a("局部加权线性回归概述 "),s("a",{class:"header-anchor",href:"#局部加权线性回归概述","aria-label":'Permalink to "局部加权线性回归概述"'},"​")],-1),s("ol",null,[s("li",null,"线性回归的以恶搞问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。"),s("li",null,[a("一个方法是局部加权线性回归，在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归，我们需要最小化的目标函数大致为："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msubsup",null,[s("mo",null,"∑"),s("mi",null,"i"),s("mrow")]),s("mi",null,"ω"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"y"),s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",{stretchy:"false"},")")])]),s("mo",null,"−"),s("msup",null,[s("mover",{accent:"true"},[s("mi",null,"y"),s("mo",null,"^")]),s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",{stretchy:"false"},")")])]),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"\\sum_{i}^{} \\omega (y^{(i)} - {\\hat y}^{(i)})^2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1877em","vertical-align":"-0.2997em"}}),s("span",{class:"mop"},[s("span",{class:"mop op-symbol small-op",style:{position:"relative",top:"0em"}},"∑"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.5029em"}},[s("span",{style:{top:"-2.4003em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{style:{top:"-3.2029em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"})])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2997em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"ω"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.888em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mclose mtight"},")")])])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.2223em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord accent"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6944em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y")]),s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.1944em"}},[s("span",{class:"mord"},"^")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1944em"}},[s("span")])])])])]),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9723em"}},[s("span",{style:{top:"-3.1473em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mclose mtight"},")")])])])])])])])]),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])])]),s("li",null,[a("目标函数中 w 为权重，不是回归系数。与 KNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解出回归系数 w 的形式如下："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mover",{accent:"true"},[s("mi",null,"w"),s("mo",null,"^")]),s("mo",null,"="),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"X"),s("mi",null,"T")]),s("mi",null,"W"),s("mi",null,"X"),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mrow",null,[s("mo",null,"−"),s("mn",null,"1")])]),s("msup",null,[s("mi",null,"X"),s("mi",null,"T")]),s("mi",null,"W"),s("mi",null,"y")]),s("annotation",{encoding:"application/x-tex"},"\\hat w = (X^TWX)^{-1}X^TWy")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord accent"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6944em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w")]),s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.1667em"}},[s("span",{class:"mord"},"^")])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0913em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8413em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y")])])]),a(" 。其中 W 是一个矩阵，用来给每个数据点赋予权重，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mover",{accent:"true"},[s("mi",null,"w"),s("mo",null,"^")])]),s("annotation",{encoding:"application/x-tex"},"\\hat w")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord accent"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6944em"}},[s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w")]),s("span",{style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"accent-body",style:{left:"-0.1667em"}},[s("span",{class:"mord"},"^")])])])])])])])])]),a("则为回归系数，这两个是不同概念")]),s("li",null,[a("LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下:"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"w"),s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mi",null,"exp"),s("mo",null,"⁡"),s("mrow",null,[s("mo",{fence:"true"},"("),s("mfrac",null,[s("mrow",null,[s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"i"),s("mo",{stretchy:"false"},")")])]),s("mo",null,"−"),s("mi",null,"x"),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mn",null,"2")])]),s("mrow",null,[s("mo",null,"−"),s("mn",null,"2"),s("msup",null,[s("mi",null,"k"),s("mn",null,"2")])])]),s("mo",{fence:"true"},")")])]),s("annotation",{encoding:"application/x-tex"},"w(i) = \\exp \\left( \\frac{(x^{(i)} - x)^2}{-2k^2} \\right)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.8117em","vertical-align":"-0.65em"}}),s("span",{class:"mop"},"exp"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size2"},"(")]),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.1617em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mtight"},"2"),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7463em"}},[s("span",{style:{top:"-2.786em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.485em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9667em"}},[s("span",{style:{top:"-2.9667em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5357em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mclose mtight"},")")])])])])])])])]),s("span",{class:"mbin mtight"},"−"),s("span",{class:"mord mathnormal mtight"},"x"),s("span",{class:"mclose mtight"},[s("span",{class:"mclose mtight"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[s("span",{class:"pstrut",style:{height:"2.5em"}}),s("span",{class:"sizing reset-size3 size1 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.4033em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mclose delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size2"},")")])])])])])]),s("li",null,"这样就构建了一个只含对角元素的权重矩阵 w，并且点 x 与 x(i) 越近，w(i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数。")],-1),s("h4",{id:"局部加权线性回归-工作原理",tabindex:"-1"},[a("局部加权线性回归 工作原理 "),s("a",{class:"header-anchor",href:"#局部加权线性回归-工作原理","aria-label":'Permalink to "局部加权线性回归 工作原理"'},"​")],-1),s("ul",null,[s("li",null,"读入数据，将数据特征x、特征标签y存储在矩阵x、y中"),s("li",null,"利用高斯核构造一个权重矩阵 W，对预测点附近的点施加权重"),s("li",null,"验证 X^TWX 矩阵是否可逆"),s("li",null,"使用最小二乘法求得 回归系数 w 的最佳估计")],-1),s("h2",{id:"reference",tabindex:"-1"},[a("Reference "),s("a",{class:"header-anchor",href:"#reference","aria-label":'Permalink to "Reference"'},"​")],-1),s("p",null,[s("a",{href:"https://github.com/remnantsaint/ailearning/blob/master/docs/ml/8.md",target:"_blank",rel:"noreferrer"},"https://github.com/remnantsaint/ailearning/blob/master/docs/ml/8.md")],-1)])]),"main-header":l(()=>[e(t.$slots,"main-header")]),"main-header-after":l(()=>[e(t.$slots,"main-header-after")]),"main-nav":l(()=>[e(t.$slots,"main-nav")]),"main-content":l(()=>[e(t.$slots,"main-content")]),"main-content-after":l(()=>[e(t.$slots,"main-content-after")]),"main-nav-before":l(()=>[e(t.$slots,"main-nav-before")]),"main-nav-after":l(()=>[e(t.$slots,"main-nav-after")]),comment:l(()=>[e(t.$slots,"comment")]),footer:l(()=>[e(t.$slots,"footer")]),aside:l(()=>[e(t.$slots,"aside")]),"aside-custom":l(()=>[e(t.$slots,"aside-custom")]),default:l(()=>[e(t.$slots,"default")]),_:3},8,["frontmatter"])}}};export{T as default};
