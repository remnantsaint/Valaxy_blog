import{_ as o}from"./ValaxyMain.vue_vue_type_style_index_0_lang-B5W-C4hy.js";import{u,c as g,o as d,w as l,r as n,g as s,h as a,f as b,p}from"./app-CiB7UBw_.js";import"./YunFooter-tBh2ornf.js";import"./YunCard.vue_vue_type_script_setup_true_lang-Bt2wnHI3.js";import"./index-C5okkQwF.js";import"./YunPageHeader.vue_vue_type_script_setup_true_lang-CE6MNawq.js";import"./post-Czwbn2Xh.js";const L={__name:"mamba 浅学",setup(v,{expose:c}){const e=JSON.parse('{"title":"Mamba 浅学","description":"","frontmatter":{"layout":"post","title":"Mamba 浅学","date":"2025-11-24 12:54:59","updated":"2025-11-25","time_warning":true,"cover":null,"top":null,"tags":["深度学习"],"categories":["深度学习"],"draft":null},"headers":[{"level":2,"title":"Mamba 动机","slug":"mamba-动机","link":"#mamba-动机","children":[{"level":3,"title":"Transformer","slug":"transformer","link":"#transformer","children":[]},{"level":3,"title":"RNN","slug":"rnn","link":"#rnn","children":[]}]},{"level":2,"title":"Mamba 背景","slug":"mamba-背景","link":"#mamba-背景","children":[{"level":3,"title":"状态空间模型（SSM）","slug":"状态空间模型-ssm","link":"#状态空间模型-ssm","children":[]},{"level":3,"title":"线性状态空间层（LSSL）","slug":"线性状态空间层-lssl","link":"#线性状态空间层-lssl","children":[]},{"level":3,"title":"结构化序列空间模型（S4）","slug":"结构化序列空间模型-s4","link":"#结构化序列空间模型-s4","children":[]}]},{"level":2,"title":"Mamba 介绍","slug":"mamba-介绍","link":"#mamba-介绍","children":[{"level":3,"title":"Mamba 要解决什么问题","slug":"mamba-要解决什么问题","link":"#mamba-要解决什么问题","children":[]},{"level":3,"title":"Mamba 的特性一：选择性的保留信息","slug":"mamba-的特性一-选择性的保留信息","link":"#mamba-的特性一-选择性的保留信息","children":[]},{"level":3,"title":"Mamba 的特性二：扫描操作","slug":"mamba-的特性二-扫描操作","link":"#mamba-的特性二-扫描操作","children":[]},{"level":3,"title":"Mamba 的特性三：硬件感知算法","slug":"mamba-的特性三-硬件感知算法","link":"#mamba-的特性三-硬件感知算法","children":[]},{"level":3,"title":"Mamba 基础块的设计","slug":"mamba-基础块的设计","link":"#mamba-基础块的设计","children":[]}]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"relativePath":"pages/posts/mamba 浅学.md","path":"/home/runner/work/Valaxy_blog/Valaxy_blog/pages/posts/mamba 浅学.md","lastUpdated":1766710824000}'),i=u(),m=e.frontmatter||{};return i.meta.frontmatter=Object.assign(i.meta.frontmatter||{},e.frontmatter||{}),p("pageData",e),p("valaxy:frontmatter",m),globalThis.$frontmatter=m,c({frontmatter:{layout:"post",title:"Mamba 浅学",date:"2025-11-24 12:54:59",updated:"2025-11-25",time_warning:!0,cover:null,top:null,tags:["深度学习"],categories:["深度学习"],draft:null}}),(t,r)=>{const h=o;return d(),g(h,{frontmatter:b(m)},{"main-content-md":l(()=>[...r[0]||(r[0]=[s("h2",{id:"mamba-动机",tabindex:"-1"},[a("Mamba 动机 "),s("a",{class:"header-anchor",href:"#mamba-动机","aria-label":'Permalink to "Mamba 动机"'},"​")],-1),s("p",null,"transformer 最大的问题是都采用了注意力机制，而注意力随序列长度的二次增长，简单来说就是需要大量单词查询的时候，transformer的速度可能会非常慢。Mamba 就是致力于解决这个问题，性能更好，而且计算量和序列长度呈线性缩放。",-1),s("h3",{id:"transformer",tabindex:"-1"},[a("Transformer "),s("a",{class:"header-anchor",href:"#transformer","aria-label":'Permalink to "Transformer"'},"​")],-1),s("p",null,"Transformer 把文本的输入看成一个包含多个单词（token）的序列（sequence）。",-1),s("p",null,"Transformer 的优点是：无论当前输入是什么，都可以回顾之前看到的所有 Token，而且可以在训练过程中进行并行化运算。",-1),s("p",null,[a("Transformer 的缺点是：在推理过程中，每生成一个新的 token 都需要重新为整个序列计算一个新的 attention map，导致推理性能很慢。对于一个长度为 L 的序列，大约需要 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"L"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"L^2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"L"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])]),a(" 的计算，如果序列长度增加，计算量会更大。")],-1),s("p",null,"Transformer 主要包括两个结构，一个是编码器（encoder）用来编码文本输入，一个是解码器（decoder）用来生成输出。",-1),s("p",null,"Transformer 性能总结：训练快、推理慢。",-1),s("p",null,"目前很火的生成式模型（如 chatGPT）是只使用 decoders 的结构。",-1),s("p",null,"Decoder 是由一个 masked self-attention 和一个 feed-forward neural network 构成。其中 self-attention 是 transformer 的核心，可以生成一个 attention map。attention map 是一个权重矩阵，用来存储每两个 token 之间相关性的大小。在训练过程中，这个 attention map 的矩阵是可以并行生成的，因此可以加速训练。",-1),s("h3",{id:"rnn",tabindex:"-1"},[a("RNN "),s("a",{class:"header-anchor",href:"#rnn","aria-label":'Permalink to "RNN"'},"​")],-1),s("p",null,"循环神经网络（RNN）是一种基于序列的网络。它在序列的每个时间步都需要两个输入，即时间步 T 的输入和前一个时间步 t-1 的隐藏状态，用来生成下一个隐藏状态并预测输入。",-1),s("p",null,"RNN的优点：在生成当前的输出时，RNN只需要考虑当前的输入和上一时刻的隐藏状态。和 Transformer 相比，RNN 不需要重新计算先前所有的隐藏状态。换句话说，RNN 可以快速进行推理，因为它的计算量与序列长度呈线性扩展。理论上，它甚至可以拥有无限长的上下文长度。RNN中，每个隐藏状态都是之前所有隐藏状态的聚合，通常是一个压缩视图。",-1),s("p",null,"RNN的缺点：训练不能并行，因为它需要按照时间顺序完成每个步骤。因为在反向传播中，RNN需要计算每个时间步的梯度，并且这些梯度需要沿着时间步依次传播回去。如果同时更新所有时间步的参数，会导致梯度混乱和不稳定，因此不能并行训练。",-1),s("p",null,"RNN的性能总结：训练慢、推理快。",-1),s("h2",{id:"mamba-背景",tabindex:"-1"},[a("Mamba 背景 "),s("a",{class:"header-anchor",href:"#mamba-背景","aria-label":'Permalink to "Mamba 背景"'},"​")],-1),s("h3",{id:"状态空间模型-ssm",tabindex:"-1"},[a("状态空间模型（SSM） "),s("a",{class:"header-anchor",href:"#状态空间模型-ssm","aria-label":'Permalink to "状态空间模型（SSM）"'},"​")],-1),s("h4",{id:"什么是状态空间",tabindex:"-1"},[a("什么是状态空间？ "),s("a",{class:"header-anchor",href:"#什么是状态空间","aria-label":'Permalink to "什么是状态空间？"'},"​")],-1),s("p",null,"举个例子，假如我们在走迷宫，那么状态空间就是我们在地图中所有可能的状态，包含（在哪里？往哪里走？下一步可能在哪里？）",-1),s("p",null,"描述状态的变量，在我们例子中是 X 和 Y 坐标，以及到出口的距离，这些参数可以表示为“状态向量”",-1),s("h4",{id:"什么是状态空间模型",tabindex:"-1"},[a("什么是状态空间模型？ "),s("a",{class:"header-anchor",href:"#什么是状态空间模型","aria-label":'Permalink to "什么是状态空间模型？"'},"​")],-1),s("p",null,"SSM 是用于描述这些状态表示的模型，并根据某些输入预测下一个状态可能是什么。",-1),s("p",null,"在时刻 t ，SSMs 为：",-1),s("ul",null,[s("li",null,"映射输入序列x(t) -(例如，在迷宫中向左和向下移动)"),s("li",null,"到隐藏状态表示h(t) -(例如，到出口的距离和x/y坐标)"),s("li",null,"并推导出预测的输出序列y(t) -(例如，再次向左移动以更快地到达出口)")],-1),s("p",null,[a("然而，SSM 不是使用离散序列（如向左移动一次），而是将连续序列作为输入，并预测输出序列。SSM 假设动态系统，例如在 3D 空间中运动的物体，可以通过两个方程从其在时间 t 的状态进行预测 "),s("img",{src:"https://cloudflare.remsait.com/img/SSM202511241543310.png",alt:""})],-1),s("h4",{id:"状态方程和输出方程",tabindex:"-1"},[a("状态方程和输出方程 "),s("a",{class:"header-anchor",href:"#状态方程和输出方程","aria-label":'Permalink to "状态方程和输出方程"'},"​")],-1),s("p",null,"状态方程：矩阵 A 和矩阵 B 分别控制着当前状态和输入如何影响状态的变化",-1),s("p",null,"输出方程：描述了状态如何转换为输出（通过矩阵 C ），以及输入如何影响输出（通过矩阵 D ）",-1),s("p",null,[a("A、B、C、D 都是科学系的参数，将上述两个方差整合在一起，得到如下结构： "),s("img",{src:"https://cloudflare.remsait.com/img/SSM202511241607381.png",alt:""})],-1),s("ul",null,[s("li",null,"假设我们有一些输入信号 x(t) ，这个信号首先乘以矩阵 B ，矩阵 B 描述了输入如何影响系统。"),s("li",null,"矩阵 A 和当前状态相乘，矩阵 A 描述了内部状态之间是如何连接的"),s("li",null,"矩阵 C 和新的状态相乘，矩阵 C 描述了状态时如何转化到输出的"),s("li",null,"矩阵D提供一个从输入到输出的直接信号，这通常也称为跳跃连接（skip-connection)")],-1),s("p",null,"SSM 通常被认为是不包含跳跃连接的部分，可以看出矩阵 A、B、C 是 SSM 的核心。由于预期输入是连续的，所以 SSM 的主要表示是连续时间表示。",-1),s("h3",{id:"线性状态空间层-lssl",tabindex:"-1"},[a("线性状态空间层（LSSL） "),s("a",{class:"header-anchor",href:"#线性状态空间层-lssl","aria-label":'Permalink to "线性状态空间层（LSSL）"'},"​")],-1),s("p",null,"LSSL 的核心思想是把连续时间的 SSM 进行离散化，得到两种离散化的表示（循环形式和卷积形式）",-1),s("h4",{id:"将连续的信号转化为离散的信号",tabindex:"-1"},[a("将连续的信号转化为离散的信号 "),s("a",{class:"header-anchor",href:"#将连续的信号转化为离散的信号","aria-label":'Permalink to "将连续的信号转化为离散的信号"'},"​")],-1),s("p",null,"通常而言，我们的输入是离散的，例如一个文本序列，为了将离散的输入变成 SSM 可用的连续信号，我们使用 零阶保持技术（Zero-order hold technique）",-1),s("p",null,"零阶保持技术的原理：我们每次接收到一个离散信号时，都保持它的值，直到我们接收到一个新的离散信号，我们保存该值的时间由一个新的可学习参数表示，称为 步长A。现在我们有了一个连续的信号作为输入，我们可以生成一个连续的输出，并且只根据输入的时间步长对值进行采样，这个采样的值就是我们离散化的输出。也就是说离散的输入成连续，连续的输出成离散。",-1),s("p",null,[a("我们从一个连续的 SSM（函数到函数，x(t) -> y(t)）到一个离散 SSM （序列到序列，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"x"),s("mi",null,"k")]),s("mo",null,"−"),s("mo",null,">"),s("msub",null,[s("mi",null,"y"),s("mi",null,"k")])]),s("annotation",{encoding:"application/x-tex"},"x_k -> y_k")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},">"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("） "),s("img",{src:"https://cloudflare.remsait.com/img/SSM202511241635333.png",alt:""})],-1),s("h4",{id:"循环表示",tabindex:"-1"},[a("循环表示 "),s("a",{class:"header-anchor",href:"#循环表示","aria-label":'Permalink to "循环表示"'},"​")],-1),s("p",null,[a("在每个时间步长，我们计算当前输入（B"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"x"),s("mi",null,"k")])]),s("annotation",{encoding:"application/x-tex"},"x_k")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("）如何影响前一个状态（"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"A"),s("msub",null,[s("mi",null,"h"),s("mrow",null,[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])])]),s("annotation",{encoding:"application/x-tex"},"Ah_{k-1}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9028em","vertical-align":"-0.2083em"}}),s("span",{class:"mord mathnormal"},"A"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"mbin mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2083em"}},[s("span")])])])])])])])]),a("），然后计算预测输出（"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"C"),s("msub",null,[s("mi",null,"h"),s("mi",null,"k")])]),s("annotation",{encoding:"application/x-tex"},"Ch_k")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8444em","vertical-align":"-0.15em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("）")],-1),s("p",null,"我们可以发现这种循环的 SSM 结构和 RNN 非常的类似。这使得我们可以将RNN的基本方法应用在离散的SSM上，但还需要考虑RNN推理快和训练慢的特性。",-1),s("h4",{id:"卷积表示",tabindex:"-1"},[a("卷积表示 "),s("a",{class:"header-anchor",href:"#卷积表示","aria-label":'Permalink to "卷积表示"'},"​")],-1),s("p",null,"在经典的图像识别任务中，我们使用卷积核来聚集特征，类似的，因为我们处理的是文本而不是图像，所以我们需要一维卷积",-1),s("p",null,"将SSM表示为卷积的一个主要好处是，它可以像卷积神经网络(CNN)一样并行训练。然而，由于核大小固定，它们的推理不像RNN那样快速。",-1),s("h4",{id:"lssl-的设计思路",tabindex:"-1"},[a("LSSL 的设计思路 "),s("a",{class:"header-anchor",href:"#lssl-的设计思路","aria-label":'Permalink to "LSSL 的设计思路"'},"​")],-1),s("p",null,"SSM的三种表示：连续时间、循环、卷积。有了这些表示，我们可以使用一个巧妙的技巧，即根据任务选择一种表示，在推理过程中，我们使用高效的循环表示，这种混合表示就被称为 LSSL。",-1),s("p",null,"LSSL的一个重要特性就是 线性时间不变（LTI）",-1),s("p",null,"LTI 声明 SSM 参数 A、B、C 对于所有时间步都是固定的，这意味着矩阵 A、B、C 对于 SSM 生成的每个 token 都是相同的，换句话说，无论你给 SSM 什么序列，A、B、C的值都保持不变，我们有一个不感知内容的静态表示。",-1),s("h3",{id:"结构化序列空间模型-s4",tabindex:"-1"},[a("结构化序列空间模型（S4） "),s("a",{class:"header-anchor",href:"#结构化序列空间模型-s4","aria-label":'Permalink to "结构化序列空间模型（S4）"'},"​")],-1),s("p",null,"Mamba 模型是基于 S4 模型构建的。S4 是一个 线性时间不变 的状态空间模型，在每个状态之间，矩阵 A 捕获关于前一个状态的信息以构建新状态，矩阵 A 本质上是用来产生隐藏状态的，那么我们如何构建矩阵 A ，使其保留更多的上下文信息？",-1),s("p",null,[a("这里使用的是 HIPPO ，HIPPO 试图将它迄今为止看到的所有输入信号压缩为一个系数向量，HIPPO 使用矩阵 A 来构建状态标识，可以很好地捕获最近的 token 并衰减旧的 token，其公式可以表示为："),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"A"),s("mrow",null,[s("mi",null,"n"),s("mi",null,"k")])]),s("mo",null,"=")]),s("annotation",{encoding:"application/x-tex"},"A_{nk} =")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"nk")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"=")])])])],-1),s("ul",null,[s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msqrt",null,[s("mrow",null,[s("mn",null,"2"),s("mi",null,"n"),s("mo",null,"+"),s("mn",null,"1")])]),s("msqrt",null,[s("mrow",null,[s("mn",null,"2"),s("mi",null,"k"),s("mo",null,"+"),s("mn",null,"1")])])]),s("annotation",{encoding:"application/x-tex"},"\\sqrt{2n+1}\\sqrt{2k+1}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.065em","vertical-align":"-0.1744em"}}),s("span",{class:"mord sqrt"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8656em"}},[s("span",{class:"svg-align",style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord",style:{"padding-left":"0.833em"}},[s("span",{class:"mord"},"2"),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1")])]),s("span",{style:{top:"-2.8256em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"hide-tail",style:{"min-width":"0.853em",height:"1.08em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"},[s("path",{d:`M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1744em"}},[s("span")])])])]),s("span",{class:"mord sqrt"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8906em"}},[s("span",{class:"svg-align",style:{top:"-3em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord",style:{"padding-left":"0.833em"}},[s("span",{class:"mord"},"2"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1")])]),s("span",{style:{top:"-2.8506em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"hide-tail",style:{"min-width":"0.853em",height:"1.08em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"},[s("path",{d:`M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1494em"}},[s("span")])])])])])])]),a("（everything below the diagonal")]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n"),s("mo",null,"+"),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"},"n + 1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1")])])]),a("（the diagonal")]),s("li",null,"0（everything above the diagonal")],-1),s("p",null,"假设矩阵 A 的大小是 4*4 ，那么它的 HIPPO Matrix 的表示如下：",-1),s("table",null,[s("thead",null,[s("tr",null,[s("th",null,"1"),s("th",null,"0"),s("th",null,"0"),s("th",null,"0")])]),s("tbody",null,[s("tr",null,[s("td",null,"1"),s("td",null,"2"),s("td",null,"0"),s("td",null,"0")]),s("tr",null,[s("td",null,"1"),s("td",null,"3"),s("td",null,"3"),s("td",null,"0")]),s("tr",null,[s("td",null,"1"),s("td",null,"3"),s("td",null,"5"),s("td",null,"4")])])],-1),s("p",null,"使用 HIPPO 构建矩阵 A 比初始化为随机矩阵要好很多，因此，与旧信号（初始 token）相比，它可以更准确地重建较新的信号（最近的 token）",-1),s("p",null,"HiPPO矩阵背后的想法是，它产生一个隐藏状态来记忆其历史。从数学的角度而言，它通过跟Legendr多项式的系数来实现这一点，这使得它可以近似之前的所有历史。然后将HiPPO应用于我们之前看到的递归和卷积表示，以处理长程依赖关系。其结果是序列的结构化状态空间(Structured State Space for Sequences, S4)，这是一类可以有效处理长序列的SSM。",-1),s("p",null,"S4 主要包括以下三个部分：状态空间模型；HIPPO 用于处理远程依赖；用于创建循环和卷积表示的离散化。",-1),s("h2",{id:"mamba-介绍",tabindex:"-1"},[a("Mamba 介绍 "),s("a",{class:"header-anchor",href:"#mamba-介绍","aria-label":'Permalink to "Mamba 介绍"'},"​")],-1),s("p",null,"Mamba 是一种状态空间模型（SSM）架构，它融合了 RNN 和 CNN 的特点通过递归或卷积操作实现计算成本与序列长度的线性或近线性扩展，改进了 S4 架构。它有时也被称为 S6 或者 selective SSM ，它对 S4 进行了两项重要修改：",-1),s("ul",null,[s("li",null,[s("strong",null,"选择性扫描算法"),a("：允许模型过滤相关或者不相关的信息，保留必要数据")]),s("li",null,[s("strong",null,"硬件感知算法"),a("：采用递归扫描而非卷积运算，允许通过并行扫描、核融合和重计算有效地存储中间结果")])],-1),s("h3",{id:"mamba-要解决什么问题",tabindex:"-1"},[a("Mamba 要解决什么问题 "),s("a",{class:"header-anchor",href:"#mamba-要解决什么问题","aria-label":'Permalink to "Mamba 要解决什么问题"'},"​")],-1),s("p",null,"SSM 和 S4 无法选择性的关注指定的输入。",-1),s("p",null,"比如说，在选择性复制任务中，SSM 的目标是复制输入的一部分并按序输出。然而，循环或者卷积 SSM 在这项任务中表现不佳，因为它是线性时间不变的，矩阵A、B、C生成的每个 token 都是相同的，因此 SSM 无法进行内容感知推理，因为它将每个 token 视为固定的 A、B、C 的结果，这是一个问题，我们希望 SSM 对输入进行推理。",-1),s("p",null,'或者是要重现在输入中发现的模式，本质上是在实行 one-shot prompting，试图“教”模型在每个“Q”之后提供一个“A"响应。然而，由于 SSM 是时不变的，它无法选择从历史中回忆之前的哪个标记。无论输入是什么，A、B、C都保持不变，这表明我们迄今为止看到的 SSM 都是静态的。',-1),s("p",null,"相比之下，Transformer 对这些任务相对容易，因为它们根据输入序列动态改变注意力，它们可以选择性地看或关注序列的不同部分。",-1),s("p",null,"SSM 在这些任务上的糟糕表现说明了潜在问题：矩阵A、B、C的静态性质导致了其无法进行内容感知。",-1),s("h3",{id:"mamba-的特性一-选择性的保留信息",tabindex:"-1"},[a("Mamba 的特性一：选择性的保留信息 "),s("a",{class:"header-anchor",href:"#mamba-的特性一-选择性的保留信息","aria-label":'Permalink to "Mamba 的特性一：选择性的保留信息"'},"​")],-1),s("p",null,"SSM 的循环表示创建了一个非常高效的小状态，因为它压缩了整个历史状态。然而与没有压缩历史状态的 Transformer 模型相比，它的性能要差很多。",-1),s("p",null,[a("Mamba 致力于保留一个小的且有用的状态信息，兼顾性能和效率，它通过"),s("strong",null,"选择地将数据压缩到状态中"),a("来实现这一点（当有一个输入句子时，通常会有一些信息，比如标点，没有太多意义。这些无意义的信息就可以被忽略掉。）")],-1),s("p",null,"为了有选择地压缩信息，我们需要参数依赖于输入，为此，让我们先探究下在训练过程中 SSM 的输入和输出维度",-1),s("p",null,"SSM 中输入和输出的维度：B 表示 batchsize，L 表示序列长度，D表示输入张量大小",-1),s("p",null,"在结构化状态空间模型（S4）中，矩阵A、B、C与输入无关，因为他们的维数 N 和 D 是静态的，不会改变",-1),s("p",null,"相反，Mamba 通过合并输入的序列长度和批次大小，使矩阵 B 和 C ，甚至步长∆依赖于输入，这意味着对于每个输入标记，我们现在有不同的 B 和 C 矩阵，这解决了内容感知问题，这样我们就可以依赖于输入，选择什么保持在隐藏状态，什么要忽略。（Mamba 中矩阵 A 保持不变，因为我们希望状态本身保持静态，但它被影响的方式（通过 B 和 C）是动态的。",-1),s("p",null,[a("更小的步长∆导致忽略特定的单词，而更多地使用前一个上下文，而更大的步长∆则更多地关注输入单词而不是上下文。（"),s("strong",null,"Mamba 通过动态调整步长实现选择性关注")],-1),s("h3",{id:"mamba-的特性二-扫描操作",tabindex:"-1"},[a("Mamba 的特性二：扫描操作 "),s("a",{class:"header-anchor",href:"#mamba-的特性二-扫描操作","aria-label":'Permalink to "Mamba 的特性二：扫描操作"'},"​")],-1),s("p",null,"上述的选择性保留信息也带来了一些问题：由于现在的 B、C、∆ 矩阵是动态的，它们不能使用卷积表示进行计算，因此它假设一个固定的核，我们只能使用递归表示，而失去了卷积提供的并行化。",-1),s("p",null,[a("为了实现并行化，要使用递归计算输出。每个状态是前一个状态（乘以A）加上当前输入（乘以B）的和，这称为扫描操作，可以用 for 循环轻松计算。 "),s("img",{src:"https://cloudflare.remsait.com/img/Mamba202511251450360.png",alt:""}),a(" 相比之下，并行化似乎是不可能的，因为只有在我们拥有前一个状态的情况下，每个状态才能计算出来，然而，Mamba 通过并行扫描算法使这成为可能。 "),s("img",{src:"https://cloudflare.remsait.com/img/Mamba202511251450411.png",alt:""})],-1),s("h3",{id:"mamba-的特性三-硬件感知算法",tabindex:"-1"},[a("Mamba 的特性三：硬件感知算法 "),s("a",{class:"header-anchor",href:"#mamba-的特性三-硬件感知算法","aria-label":'Permalink to "Mamba 的特性三：硬件感知算法"'},"​")],-1),s("p",null,"GPU 的一个缺点使它们在小型但高效的 SRAM 和大型但略低效率的 DRAM 之间的传输（IO）速度有限，频繁地在 SRAM 和 DRAM 之间复制信息成为瓶颈。",-1),s("p",null,"与 Flash Attention 一样，Mamba 试图限制从 DRAM 切换到 SRAM 的次数，反之亦然。它通过核融合来实现这一点，核融合允许模型防止写入中间结果，并持续执行计算，直到完成。",-1),s("p",null,"核融合将下列代码融合到一个内核中：用∆离散化步长、选择性扫描算法、与C相乘",-1),s("p",null,"硬件感知算法的最后一部分是重计算，中间状态不保存，但对于反向传递计算梯度是必要的，作者在反向传递期间重新计算这些中间状态。虽然这看起来效率不高，但与从相对较慢的 DRAM 读取所有中间状态相比，它的开销要小的多。",-1),s("p",null,"现在已经介绍了 Mamba 架构的所有组件，下图是 Mamba 的 Overview：",-1),s("ul",null,[s("li",null,[a("首先，输入 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"X"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"X_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 通过选择性机制映射得到 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"B"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"B_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0502em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("，∆，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"C"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"C_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])]),s("li",null,[a("然后使用 ∆，用零阶保持技术对 A 和 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"B"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"B_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0502em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 进行离散化")]),s("li",null,[a("离散化后的 B 和输入 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"X"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"X_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 相乘，离散化后的 A 和原始状态 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"h"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"−"),s("mn",null,"1")])])]),s("annotation",{encoding:"application/x-tex"},"h_{t-1}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9028em","vertical-align":"-0.2083em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mbin mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2083em"}},[s("span")])])])])])])])]),a(" 相乘，将这两项相加得到新的状态 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"h"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"h_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8444em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])])]),s("li",null,[a("新状态和 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"C"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"C_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a(" 相乘，得到输出 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"y"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"y_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),s("img",{src:"https://cloudflare.remsait.com/img/Mamba202511251514515.png",alt:""})])],-1),s("h3",{id:"mamba-基础块的设计",tabindex:"-1"},[a("Mamba 基础块的设计 "),s("a",{class:"header-anchor",href:"#mamba-基础块的设计","aria-label":'Permalink to "Mamba 基础块的设计"'},"​")],-1),s("p",null,"在 Transformer 中，用 Decoder block 来实现 self-attention。与此类似，在 Mamba 中，也使用 Mamba Block 来实现 selective SSM。",-1),s("p",null,"和解码器一样，我们可以将多个 Mamba 块堆叠起来，带有标准规范化曾和残差连接交织组成，并将它们的输出作为下一个 Mamba 块的输入。它继承了状态空间模型序列长度的线性可伸缩性，同时实现了类似于 Transformer 的建模能力。",-1),s("p",null,"它首先用一个线性投影，得到我们的输入嵌入，然后在应用选择性 SSM 之前进行卷积，以防止独立 token 计算。选择性 SSM 具有以下属性：",-1),s("ul",null,[s("li",null,"通过离散化创建循环SSM"),s("li",null,"在矩阵A上进行HiPPO初始化以捕获长程依赖关系"),s("li",null,"选择性扫描算法选择性地压缩信息"),s("li",null,"硬件感知算法加速计算")],-1),s("p",null,[a("在查看代码实现时，我们可以进一步扩展次架构，并探索端到端的示例 "),s("img",{src:"https://cloudflare.remsait.com/img/Mamba202511251530706.png",alt:""}),a(" Mamba 即可以进行并行化训练，也可以按照线性缩放的复杂度进行推理，同时可以处理无限的上下文信息。")],-1),s("h2",{id:"reference",tabindex:"-1"},[a("Reference "),s("a",{class:"header-anchor",href:"#reference","aria-label":'Permalink to "Reference"'},"​")],-1),s("p",null,[s("a",{href:"https://blog.csdn.net/zyw2002/article/details/136840829",target:"_blank",rel:"noreferrer"},"Mamba 基础讲解【SSM,LSSL,S4,S5,Mamba】")],-1),s("p",null,[s("a",{href:"https://zhuanlan.zhihu.com/p/26906183708",target:"_blank",rel:"noreferrer"},"一文读懂：Mamba 模型，transformer的挑战者")],-1),s("p",null,[s("a",{href:"https://arxiv.org/abs/2312.00752",target:"_blank",rel:"noreferrer"},"文献")],-1)])]),"main-header":l(()=>[n(t.$slots,"main-header")]),"main-header-after":l(()=>[n(t.$slots,"main-header-after")]),"main-nav":l(()=>[n(t.$slots,"main-nav")]),"main-content":l(()=>[n(t.$slots,"main-content")]),"main-content-after":l(()=>[n(t.$slots,"main-content-after")]),"main-nav-before":l(()=>[n(t.$slots,"main-nav-before")]),"main-nav-after":l(()=>[n(t.$slots,"main-nav-after")]),comment:l(()=>[n(t.$slots,"comment")]),footer:l(()=>[n(t.$slots,"footer")]),aside:l(()=>[n(t.$slots,"aside")]),"aside-custom":l(()=>[n(t.$slots,"aside-custom")]),default:l(()=>[n(t.$slots,"default")]),_:3},8,["frontmatter"])}}};export{L as default};
